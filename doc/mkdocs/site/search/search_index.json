{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Assistive-rehab \u00b6 Assistive-rehab is a framework for developing the assistive intelligence of R1 robot for clinical rehabilitation and tests. The project is being developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus . Scenarios \u00b6 A humanoid platform offers the potential of providing objective and quantitative measurements, while delivering repetitive therapy with constant monitoring. Motivated by this rationale, with the experts of Fondazione Don Gnocchi, we identified two main scenarios: rehabilitation : R1 guides a patient in performing physical exercises for the upper limbs and the torso. The robot physically shows the specific exercise to perform, while verifying the correct execution and providing a verbal feedback in real-time: The Train with Me study This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper . clinical tests : R1 explains the clinical test to the patient, by physically pointing and reaching keypoints in the environment, while verifying if the test is passed and extracting real-time metrics. The developed test is the Timed Up and Go (TUG) , with the patient having to stand up from a chair, walk for 3 meters, turn, go back to the chair and sit down: Rehabilitation: components \u00b6 The rehabilitation scenario has been developed through the following components: R1 is equipped with an Intel RealSense D435 depth camera, which provides RGB images along with depth data; acquisition and storage are carried out by skeletonRetriever and objectsPropertiesCollector respectively. skeletonRetriever merges 2D skeleton data, as acquired by yarpOpenPose , along with the depth information provided by the depth sensor. 3D skeletons are stored in the OPC. This architecture also allows to replay experiments , using the combination of yarpdatadumper and yarpdataplayer , giving to physiotherapists the possibility of computing new metrics (not run online) on the replayed experiment, and of maintaining a logbook of all the patient's exercises; the face recognition pipeline is responsible for associating a labeled face to a skeleton, which thus becomes unambiguously tagged. This allows potentially to personalize exercises according to the specific rehabilitation path of a patient; the analysis of motion extracts in real-time salient metrics, as the Range of Motion (RoM) of the articular angles (for abduction and internal/external rotation), and the speed of the end-point (for a reaching task). The module also exports relevant data for enabling offline reporting of the experiments at the end of the session. The motion analysis provides a useful tool to physiotherapists for both evaluating the quality of movement of a patient and benefiting from the offline report as required documentation to produce after a session; the final feedback is produced by means of an action recognizer and a feedback analysis. The action recognizer uses 2D keypoints trajectories to predict the label of the performed exercise and triggers the analysis only if the predicted label is the same as the exercise to perform. If so, the analysis compares the observed skeleton with a (predefined) template skeleton to produce real-time feedback on the range of motion, the speed and how well the target is reached; the robot arms' movement is controlled through ctpService ; the interactionManager manages the interaction between all the modules involved. Clinical tests: components \u00b6 The clinical test scenario consists of the following components: We re-used some of the components developed for the rehabilitation scenario, specifically the acquisition and storage and the motion analysis, which was extended in order to cover the lower limbs and to extract metrics as the step length and width, the walking speed and the number of steps. Additionally, we developed new components in order to enrich the human-robot interaction and the robot's perception: a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path. In such a system, the environment is supposed to be free from obstacles; a speech recognition and interpretation pipeline, which relies on the Google services API. The pipeline receives the sound from an external microphone and 1. retrieves the speech transcript from Google Speech cloud services and 2. analyses such transcript to retrieve the sentence structure and meaning, relying on Google Language Cloud services. Such system provides a flexible and natural interaction, given by the capability to interpret the question, rather than simply recognize it.; a speech trigger system based on the use of a Mystrom wifi button : when pressed, the button is configured to produce a http request handled by means of a node.js , which triggers the speech pipeline; a visual line detection, which detects start and finish lines composed of specific ArUco markers placed on the floor. Such lines indicate the start and the end of the path to the patient, while being fixed reference points for the robot; the robot points to the reference lines using its arms through ctpService ; the managerTUG manages the interaction between all the modules involved.","title":"Home"},{"location":"#assistive-rehab","text":"Assistive-rehab is a framework for developing the assistive intelligence of R1 robot for clinical rehabilitation and tests. The project is being developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus .","title":"Assistive-rehab"},{"location":"#scenarios","text":"A humanoid platform offers the potential of providing objective and quantitative measurements, while delivering repetitive therapy with constant monitoring. Motivated by this rationale, with the experts of Fondazione Don Gnocchi, we identified two main scenarios: rehabilitation : R1 guides a patient in performing physical exercises for the upper limbs and the torso. The robot physically shows the specific exercise to perform, while verifying the correct execution and providing a verbal feedback in real-time: The Train with Me study This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper . clinical tests : R1 explains the clinical test to the patient, by physically pointing and reaching keypoints in the environment, while verifying if the test is passed and extracting real-time metrics. The developed test is the Timed Up and Go (TUG) , with the patient having to stand up from a chair, walk for 3 meters, turn, go back to the chair and sit down:","title":"Scenarios"},{"location":"#rehabilitation-components","text":"The rehabilitation scenario has been developed through the following components: R1 is equipped with an Intel RealSense D435 depth camera, which provides RGB images along with depth data; acquisition and storage are carried out by skeletonRetriever and objectsPropertiesCollector respectively. skeletonRetriever merges 2D skeleton data, as acquired by yarpOpenPose , along with the depth information provided by the depth sensor. 3D skeletons are stored in the OPC. This architecture also allows to replay experiments , using the combination of yarpdatadumper and yarpdataplayer , giving to physiotherapists the possibility of computing new metrics (not run online) on the replayed experiment, and of maintaining a logbook of all the patient's exercises; the face recognition pipeline is responsible for associating a labeled face to a skeleton, which thus becomes unambiguously tagged. This allows potentially to personalize exercises according to the specific rehabilitation path of a patient; the analysis of motion extracts in real-time salient metrics, as the Range of Motion (RoM) of the articular angles (for abduction and internal/external rotation), and the speed of the end-point (for a reaching task). The module also exports relevant data for enabling offline reporting of the experiments at the end of the session. The motion analysis provides a useful tool to physiotherapists for both evaluating the quality of movement of a patient and benefiting from the offline report as required documentation to produce after a session; the final feedback is produced by means of an action recognizer and a feedback analysis. The action recognizer uses 2D keypoints trajectories to predict the label of the performed exercise and triggers the analysis only if the predicted label is the same as the exercise to perform. If so, the analysis compares the observed skeleton with a (predefined) template skeleton to produce real-time feedback on the range of motion, the speed and how well the target is reached; the robot arms' movement is controlled through ctpService ; the interactionManager manages the interaction between all the modules involved.","title":"Rehabilitation: components"},{"location":"#clinical-tests-components","text":"The clinical test scenario consists of the following components: We re-used some of the components developed for the rehabilitation scenario, specifically the acquisition and storage and the motion analysis, which was extended in order to cover the lower limbs and to extract metrics as the step length and width, the walking speed and the number of steps. Additionally, we developed new components in order to enrich the human-robot interaction and the robot's perception: a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path. In such a system, the environment is supposed to be free from obstacles; a speech recognition and interpretation pipeline, which relies on the Google services API. The pipeline receives the sound from an external microphone and 1. retrieves the speech transcript from Google Speech cloud services and 2. analyses such transcript to retrieve the sentence structure and meaning, relying on Google Language Cloud services. Such system provides a flexible and natural interaction, given by the capability to interpret the question, rather than simply recognize it.; a speech trigger system based on the use of a Mystrom wifi button : when pressed, the button is configured to produce a http request handled by means of a node.js , which triggers the speech pipeline; a visual line detection, which detects start and finish lines composed of specific ArUco markers placed on the floor. Such lines indicate the start and the end of the path to the patient, while being fixed reference points for the robot; the robot points to the reference lines using its arms through ctpService ; the managerTUG manages the interaction between all the modules involved.","title":"Clinical tests: components"},{"location":"TUG/","text":"TUG \u00b6 This application has been developed during 2019 with the aim of expanding the handled scenarios to clinical test beyond rehabilitation. The Timed Up and Go (TUG) measures the time the patient takes to stand up from a chair, walk for 3 meters and cross a line on the floor, turn, go back to the chair and sit down, as shown in the following picture: Note This was realized within the simulation environment gazebo . The application includes the following steps: R1 engages the user, proposing a clinical test of the lower limbs, specifically the TUG; R1 explains the test, navigating the environment to reach the lines on the floor and pointing at them; R1 analyzes in real-time the correct execution of the test, verifying that the user crosses the finish line and goes back to the chair and simultaneously extracting real-time walking metrics (step length and width, walking speed and number of steps); R1 eventually replies to questions during the test explanation and / or execution, if the user presses the provided button; R1 produces a final report of the test, which includes the evaluated walking metrics. Note The code implementing this application is tagged as v0.5.0 .","title":"TUG"},{"location":"TUG/#tug","text":"This application has been developed during 2019 with the aim of expanding the handled scenarios to clinical test beyond rehabilitation. The Timed Up and Go (TUG) measures the time the patient takes to stand up from a chair, walk for 3 meters and cross a line on the floor, turn, go back to the chair and sit down, as shown in the following picture: Note This was realized within the simulation environment gazebo . The application includes the following steps: R1 engages the user, proposing a clinical test of the lower limbs, specifically the TUG; R1 explains the test, navigating the environment to reach the lines on the floor and pointing at them; R1 analyzes in real-time the correct execution of the test, verifying that the user crosses the finish line and goes back to the chair and simultaneously extracting real-time walking metrics (step length and width, walking speed and number of steps); R1 eventually replies to questions during the test explanation and / or execution, if the user presses the provided button; R1 produces a final report of the test, which includes the evaluated walking metrics. Note The code implementing this application is tagged as v0.5.0 .","title":"TUG"},{"location":"Y1M5/","text":"Y1M5 \u00b6 This application has been developed during the first semester of 2018 with the aim of starting implementing the features required to support R1 as artificial assistant of the physiotherapist. The fundamental aspects considered are the following: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; the exercise the user has to perform is shown on the screen, through a template skeleton; R1 analyzes in real-time the execution of the exercise, providing a verbal feedback limited to three levels of quality of the movement; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; the physical interaction between R1 and the patient is limited to the motion of R1's head. Note The code implementing this application is tagged as v0.1.0 .","title":"Y1M5"},{"location":"Y1M5/#y1m5","text":"This application has been developed during the first semester of 2018 with the aim of starting implementing the features required to support R1 as artificial assistant of the physiotherapist. The fundamental aspects considered are the following: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; the exercise the user has to perform is shown on the screen, through a template skeleton; R1 analyzes in real-time the execution of the exercise, providing a verbal feedback limited to three levels of quality of the movement; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; the physical interaction between R1 and the patient is limited to the motion of R1's head. Note The code implementing this application is tagged as v0.1.0 .","title":"Y1M5"},{"location":"Y1Q3/","text":"Y1Q3 \u00b6 This application has been developed during the second semester of 2018, with the aim of significantly improving Y1M5 by considering the following aspects: increasing the motor functioning of R1: R1 physically shows to the patient the movement to replicate, removing the need of visualizing the movement on the screen. This improvement is intended to support the interaction between robot and patient, promoting the use of a robotic platform rather than a simple fixed camera, with the final aim of improving the quality of rehabilitation of a patient; extending the motion analysis to the end-point: the trajectory of the end-point (hand) is evaluated during a reaching task, including also the speed profile; extending the provided feedback: the feedback includes important properties of the movement, such as the speed and the range of motion, improving significantly the capability of correction. Including the mentioned point, the application considers the following steps: R1 engages the user, proposing a rehabilitative exercise of the upper limbs; R1 physically shows the exercise the user has to perform from a predefined repertoire : Abduction Internal rotation External rotation R1 analyzes in real-time the execution of the exercise, providing an extended verbal feedback which includes the speed of the movement, the range of motion and how well the target has been reached ; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; Note The code implementing this application is tagged as v0.2.0 .","title":"Y1Q3"},{"location":"Y1Q3/#y1q3","text":"This application has been developed during the second semester of 2018, with the aim of significantly improving Y1M5 by considering the following aspects: increasing the motor functioning of R1: R1 physically shows to the patient the movement to replicate, removing the need of visualizing the movement on the screen. This improvement is intended to support the interaction between robot and patient, promoting the use of a robotic platform rather than a simple fixed camera, with the final aim of improving the quality of rehabilitation of a patient; extending the motion analysis to the end-point: the trajectory of the end-point (hand) is evaluated during a reaching task, including also the speed profile; extending the provided feedback: the feedback includes important properties of the movement, such as the speed and the range of motion, improving significantly the capability of correction. Including the mentioned point, the application considers the following steps: R1 engages the user, proposing a rehabilitative exercise of the upper limbs; R1 physically shows the exercise the user has to perform from a predefined repertoire : Abduction Internal rotation External rotation R1 analyzes in real-time the execution of the exercise, providing an extended verbal feedback which includes the speed of the movement, the range of motion and how well the target has been reached ; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; Note The code implementing this application is tagged as v0.2.0 .","title":"Y1Q3"},{"location":"Y2Q2/","text":"Y2Q2 \u00b6 This application has been developed during the first semester of 2019, with the aim of enlarging the scenarios to include clinical tests (besides rehabilitation exercises as in Y1Q3 ). Such scenarios highlight the advantage of using a robot, given by the autonomous execution of the test (which is highly repetitive for physiotherapists) and the storage of quantitative results. We started with the Timed Up and Go (TUG), which measures the time the patient takes to get up from a chair, walk for 3 meters, turn around, go back to the chair and seat again. While it's easy for a physiotherapist to measure the execution time, it's much more complicated to evaluate limbs metrics during the test (such as range of motion, step length), unless of equipping the patient and the room with sensors. In such scenario, the robot has to: explain the test to the patient; naturally interact with the patient and reply to his / her questions; navigate in the environment; monitor the patient while doing the test; extract real-time metrics of the lower limbs and export them in a final report for further analysis.","title":"Y2Q2"},{"location":"Y2Q2/#y2q2","text":"This application has been developed during the first semester of 2019, with the aim of enlarging the scenarios to include clinical tests (besides rehabilitation exercises as in Y1Q3 ). Such scenarios highlight the advantage of using a robot, given by the autonomous execution of the test (which is highly repetitive for physiotherapists) and the storage of quantitative results. We started with the Timed Up and Go (TUG), which measures the time the patient takes to get up from a chair, walk for 3 meters, turn around, go back to the chair and seat again. While it's easy for a physiotherapist to measure the execution time, it's much more complicated to evaluate limbs metrics during the test (such as range of motion, step length), unless of equipping the patient and the room with sensors. In such scenario, the robot has to: explain the test to the patient; naturally interact with the patient and reply to his / her questions; navigate in the environment; monitor the patient while doing the test; extract real-time metrics of the lower limbs and export them in a final report for further analysis.","title":"Y2Q2"},{"location":"about/","text":"assistive-rehab has been developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus . Maintainer \u00b6 Vasco Valentina ( @vvasco ) Contributors \u00b6 alphabetical order main components Ugo Pattacini ( @pattacini ) Skeletons Handling Vadim Tikhanoff ( @vtikha ) Face Recognition, I/F to OpenPose Valentina Vasco ( @vvasco ) Action Recognition, Movements Analysis Special Thanks \u00b6 alphabetical order reason Diego Ferigo ( @diegoferigo ) Hints on the website","title":"About"},{"location":"about/#maintainer","text":"Vasco Valentina ( @vvasco )","title":"Maintainer"},{"location":"about/#contributors","text":"alphabetical order main components Ugo Pattacini ( @pattacini ) Skeletons Handling Vadim Tikhanoff ( @vtikha ) Face Recognition, I/F to OpenPose Valentina Vasco ( @vvasco ) Action Recognition, Movements Analysis","title":"Contributors"},{"location":"about/#special-thanks","text":"alphabetical order reason Diego Ferigo ( @diegoferigo ) Hints on the website","title":"Special Thanks"},{"location":"align_signals/","text":"How to temporally align two signals \u00b6 Some applications require signals to be temporally aligned. For example, when analyzing and comparing the current and the template skeleton for producing a feedback, joints must be aligned. Given two 1D signals, whose temporal samples are stored in vectors v1 and v2 , you can use the following code snippet to get the aligned versions w1 and w2 : assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < double > w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Tip In the example, no adjustment window condition is applied. Therefore the search of the warping path is done along the whole distance matrix. To limit the search, you can create the obejct dtw by passing the desired window, e.g. assistive_rehab::Dtw dtw(8) . The following images show two signals before and after the application of DTW: Before DTW After DTW For the multidimensional case, the code can be adapted as following: assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < std :: vector < double >> w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Note v1 and v2 are defined as std::vector<std::vector> .","title":"How to temporally align two signals"},{"location":"align_signals/#how-to-temporally-align-two-signals","text":"Some applications require signals to be temporally aligned. For example, when analyzing and comparing the current and the template skeleton for producing a feedback, joints must be aligned. Given two 1D signals, whose temporal samples are stored in vectors v1 and v2 , you can use the following code snippet to get the aligned versions w1 and w2 : assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < double > w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Tip In the example, no adjustment window condition is applied. Therefore the search of the warping path is done along the whole distance matrix. To limit the search, you can create the obejct dtw by passing the desired window, e.g. assistive_rehab::Dtw dtw(8) . The following images show two signals before and after the application of DTW: Before DTW After DTW For the multidimensional case, the code can be adapted as following: assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < std :: vector < double >> w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Note v1 and v2 are defined as std::vector<std::vector> .","title":"How to temporally align two signals"},{"location":"comparison_releases/","text":"Comparison between Y1Q3 and Y1M5 \u00b6 Acquisition \u00b6 The improvements that Y1Q3 introduces in terms of acquisition are listed below: Filtering the disparity map ; Optimizing the skeleton ; Reducing latencies . Filtering the disparity map \u00b6 The disparity map provided by the camera is inaccurate around the human contour and might have holes, leading keypoints close to the contour (e.g. hands) or on an hole to be projected incorrectly. For example, the following video shows the depth map for an abduction movement, where the hand is projected to infinite: In Y1M5, the disparity map is used as provided by the camera, without any additional processing, and thus it is affected by the effect described. In Y1Q3, the disparity map is eroded (and thus the depth map dilated), which has the double effect of: increasing the probability for keypoints closer to the contour to fall within the correct depth map; filling holes in the depth. The following video shows the effect of filtering the depth map and keypoints falling within the correct depth: Unprocessed depth (Y1M5) Filtered depth (Y1Q3) Keypoints inside the depth Optimizing the skeleton \u00b6 Exercises that require movements parallel to the optical axis make some keypoints ambiguous. For example, in the external and internal rotation, the elbow is not directly observable as the hand occludes it. Therefore both keypoints are projected to the same depth, as shown here: Without optimization (Y1M5) In Y1Q3, we introduce an optimization of the skeleton, which adjusts the depth of the keypoints such that the length of the arms is equal to that observed during an initial phase. The following image shows the result of the optimization, in which elbow and hand are projected correctly. With optimization (Y1Q3) Reducing latencies \u00b6 yarpOpenPose introduces a visible latency in the depth map, as shown here: Without yarpOpenPose With yarpOpenPose In Y1Q3, yarpOpenPose propagates the depth image in sync with the output of skeleton detection, in order to equalize the delay between the two streams. Extending the feedback \u00b6 Y1M5 \u00b6 In Y1M5, the feedback is based on the definition of dynamic joints (i.e. joints performing the exercise (1)) and static joints (i.e. joints staying in the initial position (2)): dynamic joints contribute to the dynamic score, which computes their range of motion in a predefined temporal window, by awarding joints moving within the correct range of motion; static joints contribute to the static score, which computes their deviation from a predefined resting position. Dynamic and static scores are combined to produce three levels: low: both dynamic and static scores are low, producing the following feedback: \"You are not moving very well!\" medium: either dynamic or static score is medium, producing the following feedback: \"You are doing the exercise correctly, but you could do it better.\" high: both dynamic and static scores are high, producing the following feedback: \"You are moving very well!\" Failure the feedback does not provide a real correction of the movement; the speed of the movement is not directly taken into account; the feedback can be positive even when a wrong movement is performed (for example, a different movement with the same moving joints as external rotation instead of internal and viceversa, or a random movement with dynamic joints moving within the range of motion and static joints not deviating enough from the resting position). Y1Q3 \u00b6 To overcome the drawbacks of Y1M5, in Y1Q3 the concept of dynamic and static joints is removed and the exercise is treated in its entirety as an action. Therefore the feedback is produced based on the following two layers: Action recognition : which classifies the exercise and triggers the second layer if the predicted class equals the label of the exercise to perform. Otherwise the feedback produced is negative. This layer is fundamental as it guarantees a random or a wrong movement to not produce a positive feedback. Joint analysis : which compares the observed skeleton with a predefined template for that movement, differently based on the exercise. The comparison with a template skeleton allows us to analyze also the speed of the movement. With this architecture, the feedback is extended and articulated in the following levels: positive feedback: \"You are moving very well!\" feedback for range of motion exercises: feedback on the speed: \"Move the arm faster/slower!\" feedback on the range of motion: \"Move the arm further up/down!\" feedback for reaching exercises: \"You are not reaching the target!\" negative feedback: \"You are doing the wrong exercise. Please, repeat the movements I show you.\" Action Recognition \u00b6 The action recognition is carried out using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells . Input to the network : 2D joints of the upper body from the skeletonRetriever . The output of the skeletonRetriever is preferable to that of yarpOpenPose , as the skeletonRetriever unambiguously identifies a skeleton by tag, avoiding ambiguities due to the presence of several skeletons. Preprocessing : The following operations are applied to the skeleton: normalization, such that the length of each segment is 1. This prevents having different results for different physiques; rototranslation, derived by the first observed skeleton; scale by a factor of 10, in order to have values between -1.0,1.0. Training set : 1 subject performs the following 6 exercises (i.e. 6 classes): abduction-left ; internal-rotation-left ; external-rotation-left ; reaching-left ; static (the subject remains steady); random (the subject moves randomly). The first 3 movements are repeated 10 times and the full exercise repeated 5 times. For the 4 th movement, 4 targets to be reached are defined, distributed on the corners/center of a square, centered around the shoulderLeft of the subject. Each dataset was recorded from a frontal and a side view and can be found at this link . Parameters used for training are the following: n_hidden = 22 n_steps (temporal window) = 30 learning_rate = 0.0005 batch_size = 256 epochs = 400 Validation set: The same subject, but previously unseen data, were used for testing the network. Accuracy: We get an accuracy of 92.2% with the following confusion matrix: Joint analysis \u00b6 This analysis is differentiated according to the exercises, which are classified as: range of motion exercises (i.e. abduction, internal and external rotation); reaching exercises. Range of Motion exercises \u00b6 The joints under analysis for these movements are: elbowLeft , handLeft . These movements can produce two feedbacks, i.e. on the speed and on the range of motion. The feedback is provided according to a predefined hierarchy which prioritizes the speed, followed by the range of motion. Therefore, a positive feedback is produced only when both checks are fine. Speed: Fourier analysis We perform the Fourier transform of each component of the joints under analysis in a predefined temporal window, for both the observed and the template skeleton. The difference in frequency is computed as df = f_skeleton - f_template and thus we can have two possible cases: df > 0 => feedback: \"Move the arm faster\" df < 0 => feedback: \"Move the arm slower\" Range of motion: Dynamic Time Warping (DTW) plus error statistical analysis The DTW is applied to each component of the joints under analysis, for both the observed and the template skeleton, allowing us to temporally align the signals to compare. Once joints are aligned, the error between the observed and template joints under analysis is computed. A statistical analysis is carried out, which looks for tails in the error distribution. Tails can be identified using the skewness of the distribution. Three cases can be identified: the skeleton and the template are moving similarly. Therefore the error in position is close to 0, generating a distribution centered around 0 (i.e. with low skewness): the skeleton has an higher range of motion than the template. Therefore the error will be positive, generating a distribution with a positive tail (i.e. with a positive skewness): the skeleton has a lower range of motion than the template. Therefore the error will be negative, generating a distribution with a negative tail (i.e. with a negative skewness): Reaching \u00b6 The joint under analysis for this movement is handLeft . This movement produces a feedback related to how well a predefined target is reached. Statistical analysis of the reached target We define a sphere around the predefined target and consider the points of the template which fall within the sphere. Statistics of the distribution of the template points are extracted to describe how the points should be distributed around the target. We then compute the number of points of the observed skeleton which fall within the template distribution. If the number of inliers is above a predefined threshold, the target is considered reached and a positive feedback is produced, otherwise the target is considered not reached. Comparison \u00b6 The following tables compare feedbacks produced by the two pipelines developed in Y1M5 and in Y1Q3 respectively, in response to the same movement. Corrrect feedbacks are highlighted. abduction-left Y1M5 Y1Q3 Correct 1. You are moving very well! 2. You are moving very well! 1. You are moving very well! 2. You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1. Move the left arm slower! 2. Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Low ROM 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. external-rotation-left Y1M5 Y1Q3 Correct 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. Move the left arm slower! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1. You are doing the exercise correctly, but you could do it better. 2. You are moving very well! 1. You are moving very well! 2. Move the left arm faster! Low ROM 1.You are moving very well! 2. You are doing the exercise correctly, but you could do it better. 1.You are moving very well! 2.Move the left arm slower! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. internal-rotation-left Y1M5 Y1Q3 Correct 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.Move the left arm faster! Low ROM 1.You are not moving very well! 2.You are not moving very well! 1. Move the left arm backwards! 2. Move the left arm backwards! Wrong 1.You are not moving very well! 2.You are not moving very well! 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. Success in Y1Q3 the correct feedback is provided for fast movements, while in Y1M5 the feedback is always positive, regardless the speed; in Y1Q3 the correct feedback is provided for low ROM movements, while in Y1M5 the feedback is always positive (except for the external rotation, which generates a medium score since there are time instants where the range of motion is below the minimum one); in Y1Q3 a negative feedback is provided for wrong movements, while in Y1M5 the feedback is generated by a medium score, (except for the internal rotation, due to the static score, which is low as static joints deviate a lot from the resting position). Failure in Y1Q3 a positive feedback is provided for slow movements. This occurs as the template skeleton moves slowly to emulate the movement shown by the robot and the feedback on the speed is triggered only by a very slow execution of the exercise; in Y1Q3 a feedback on the speed is provided for an external rotation with low ROM , as the threshold on the frequency is set low to trigger also a feedback on slow movements.","title":"Comparison between Y1Q3 and Y1M5"},{"location":"comparison_releases/#comparison-between-y1q3-and-y1m5","text":"","title":"Comparison between Y1Q3 and Y1M5"},{"location":"comparison_releases/#acquisition","text":"The improvements that Y1Q3 introduces in terms of acquisition are listed below: Filtering the disparity map ; Optimizing the skeleton ; Reducing latencies .","title":"Acquisition"},{"location":"comparison_releases/#filtering-the-disparity-map","text":"The disparity map provided by the camera is inaccurate around the human contour and might have holes, leading keypoints close to the contour (e.g. hands) or on an hole to be projected incorrectly. For example, the following video shows the depth map for an abduction movement, where the hand is projected to infinite: In Y1M5, the disparity map is used as provided by the camera, without any additional processing, and thus it is affected by the effect described. In Y1Q3, the disparity map is eroded (and thus the depth map dilated), which has the double effect of: increasing the probability for keypoints closer to the contour to fall within the correct depth map; filling holes in the depth. The following video shows the effect of filtering the depth map and keypoints falling within the correct depth: Unprocessed depth (Y1M5) Filtered depth (Y1Q3) Keypoints inside the depth","title":"Filtering the disparity map"},{"location":"comparison_releases/#optimizing-the-skeleton","text":"Exercises that require movements parallel to the optical axis make some keypoints ambiguous. For example, in the external and internal rotation, the elbow is not directly observable as the hand occludes it. Therefore both keypoints are projected to the same depth, as shown here: Without optimization (Y1M5) In Y1Q3, we introduce an optimization of the skeleton, which adjusts the depth of the keypoints such that the length of the arms is equal to that observed during an initial phase. The following image shows the result of the optimization, in which elbow and hand are projected correctly. With optimization (Y1Q3)","title":"Optimizing the skeleton"},{"location":"comparison_releases/#reducing-latencies","text":"yarpOpenPose introduces a visible latency in the depth map, as shown here: Without yarpOpenPose With yarpOpenPose In Y1Q3, yarpOpenPose propagates the depth image in sync with the output of skeleton detection, in order to equalize the delay between the two streams.","title":"Reducing latencies"},{"location":"comparison_releases/#extending-the-feedback","text":"","title":"Extending the feedback"},{"location":"comparison_releases/#y1m5","text":"In Y1M5, the feedback is based on the definition of dynamic joints (i.e. joints performing the exercise (1)) and static joints (i.e. joints staying in the initial position (2)): dynamic joints contribute to the dynamic score, which computes their range of motion in a predefined temporal window, by awarding joints moving within the correct range of motion; static joints contribute to the static score, which computes their deviation from a predefined resting position. Dynamic and static scores are combined to produce three levels: low: both dynamic and static scores are low, producing the following feedback: \"You are not moving very well!\" medium: either dynamic or static score is medium, producing the following feedback: \"You are doing the exercise correctly, but you could do it better.\" high: both dynamic and static scores are high, producing the following feedback: \"You are moving very well!\" Failure the feedback does not provide a real correction of the movement; the speed of the movement is not directly taken into account; the feedback can be positive even when a wrong movement is performed (for example, a different movement with the same moving joints as external rotation instead of internal and viceversa, or a random movement with dynamic joints moving within the range of motion and static joints not deviating enough from the resting position).","title":"Y1M5"},{"location":"comparison_releases/#y1q3","text":"To overcome the drawbacks of Y1M5, in Y1Q3 the concept of dynamic and static joints is removed and the exercise is treated in its entirety as an action. Therefore the feedback is produced based on the following two layers: Action recognition : which classifies the exercise and triggers the second layer if the predicted class equals the label of the exercise to perform. Otherwise the feedback produced is negative. This layer is fundamental as it guarantees a random or a wrong movement to not produce a positive feedback. Joint analysis : which compares the observed skeleton with a predefined template for that movement, differently based on the exercise. The comparison with a template skeleton allows us to analyze also the speed of the movement. With this architecture, the feedback is extended and articulated in the following levels: positive feedback: \"You are moving very well!\" feedback for range of motion exercises: feedback on the speed: \"Move the arm faster/slower!\" feedback on the range of motion: \"Move the arm further up/down!\" feedback for reaching exercises: \"You are not reaching the target!\" negative feedback: \"You are doing the wrong exercise. Please, repeat the movements I show you.\"","title":"Y1Q3"},{"location":"comparison_releases/#action-recognition","text":"The action recognition is carried out using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells . Input to the network : 2D joints of the upper body from the skeletonRetriever . The output of the skeletonRetriever is preferable to that of yarpOpenPose , as the skeletonRetriever unambiguously identifies a skeleton by tag, avoiding ambiguities due to the presence of several skeletons. Preprocessing : The following operations are applied to the skeleton: normalization, such that the length of each segment is 1. This prevents having different results for different physiques; rototranslation, derived by the first observed skeleton; scale by a factor of 10, in order to have values between -1.0,1.0. Training set : 1 subject performs the following 6 exercises (i.e. 6 classes): abduction-left ; internal-rotation-left ; external-rotation-left ; reaching-left ; static (the subject remains steady); random (the subject moves randomly). The first 3 movements are repeated 10 times and the full exercise repeated 5 times. For the 4 th movement, 4 targets to be reached are defined, distributed on the corners/center of a square, centered around the shoulderLeft of the subject. Each dataset was recorded from a frontal and a side view and can be found at this link . Parameters used for training are the following: n_hidden = 22 n_steps (temporal window) = 30 learning_rate = 0.0005 batch_size = 256 epochs = 400 Validation set: The same subject, but previously unseen data, were used for testing the network. Accuracy: We get an accuracy of 92.2% with the following confusion matrix:","title":"Action Recognition"},{"location":"comparison_releases/#joint-analysis","text":"This analysis is differentiated according to the exercises, which are classified as: range of motion exercises (i.e. abduction, internal and external rotation); reaching exercises.","title":"Joint analysis"},{"location":"comparison_releases/#range-of-motion-exercises","text":"The joints under analysis for these movements are: elbowLeft , handLeft . These movements can produce two feedbacks, i.e. on the speed and on the range of motion. The feedback is provided according to a predefined hierarchy which prioritizes the speed, followed by the range of motion. Therefore, a positive feedback is produced only when both checks are fine. Speed: Fourier analysis We perform the Fourier transform of each component of the joints under analysis in a predefined temporal window, for both the observed and the template skeleton. The difference in frequency is computed as df = f_skeleton - f_template and thus we can have two possible cases: df > 0 => feedback: \"Move the arm faster\" df < 0 => feedback: \"Move the arm slower\" Range of motion: Dynamic Time Warping (DTW) plus error statistical analysis The DTW is applied to each component of the joints under analysis, for both the observed and the template skeleton, allowing us to temporally align the signals to compare. Once joints are aligned, the error between the observed and template joints under analysis is computed. A statistical analysis is carried out, which looks for tails in the error distribution. Tails can be identified using the skewness of the distribution. Three cases can be identified: the skeleton and the template are moving similarly. Therefore the error in position is close to 0, generating a distribution centered around 0 (i.e. with low skewness): the skeleton has an higher range of motion than the template. Therefore the error will be positive, generating a distribution with a positive tail (i.e. with a positive skewness): the skeleton has a lower range of motion than the template. Therefore the error will be negative, generating a distribution with a negative tail (i.e. with a negative skewness):","title":"Range of Motion exercises"},{"location":"comparison_releases/#reaching","text":"The joint under analysis for this movement is handLeft . This movement produces a feedback related to how well a predefined target is reached. Statistical analysis of the reached target We define a sphere around the predefined target and consider the points of the template which fall within the sphere. Statistics of the distribution of the template points are extracted to describe how the points should be distributed around the target. We then compute the number of points of the observed skeleton which fall within the template distribution. If the number of inliers is above a predefined threshold, the target is considered reached and a positive feedback is produced, otherwise the target is considered not reached.","title":"Reaching"},{"location":"comparison_releases/#comparison","text":"The following tables compare feedbacks produced by the two pipelines developed in Y1M5 and in Y1Q3 respectively, in response to the same movement. Corrrect feedbacks are highlighted. abduction-left Y1M5 Y1Q3 Correct 1. You are moving very well! 2. You are moving very well! 1. You are moving very well! 2. You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1. Move the left arm slower! 2. Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Low ROM 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. external-rotation-left Y1M5 Y1Q3 Correct 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. Move the left arm slower! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1. You are doing the exercise correctly, but you could do it better. 2. You are moving very well! 1. You are moving very well! 2. Move the left arm faster! Low ROM 1.You are moving very well! 2. You are doing the exercise correctly, but you could do it better. 1.You are moving very well! 2.Move the left arm slower! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. internal-rotation-left Y1M5 Y1Q3 Correct 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.Move the left arm faster! Low ROM 1.You are not moving very well! 2.You are not moving very well! 1. Move the left arm backwards! 2. Move the left arm backwards! Wrong 1.You are not moving very well! 2.You are not moving very well! 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. Success in Y1Q3 the correct feedback is provided for fast movements, while in Y1M5 the feedback is always positive, regardless the speed; in Y1Q3 the correct feedback is provided for low ROM movements, while in Y1M5 the feedback is always positive (except for the external rotation, which generates a medium score since there are time instants where the range of motion is below the minimum one); in Y1Q3 a negative feedback is provided for wrong movements, while in Y1M5 the feedback is generated by a medium score, (except for the internal rotation, due to the static score, which is low as static joints deviate a lot from the resting position). Failure in Y1Q3 a positive feedback is provided for slow movements. This occurs as the template skeleton moves slowly to emulate the movement shown by the robot and the feedback on the speed is triggered only by a very slow execution of the exercise; in Y1Q3 a feedback on the speed is provided for an external rotation with low ROM , as the threshold on the frequency is set low to trigger also a feedback on slow movements.","title":"Comparison"},{"location":"create_new_skeleton/","text":"How to manage a skeleton object \u00b6 This tutorial will explain you the main functionalities to manage a skeleton object for: defining a skeleton; retrieving a skeleton; accessing single keypoints of a skeleton; dealing with pixels; additional functionalities. Defining a skeleton \u00b6 A skeleton can be defined as: unordered list of keypoints : the list defines the keypoints as pairs of points associated to the keypoint tag; ordered list of keypoints : the list defines the keypoints according to the following order: 0: shoulder_center 1: head 2: shoulder_left 3: elbow_left 4: hand_left 5: shoulder_right 6: elbow_right 7: hand_right 8: hip_center 9: hip_left 10: knee_left 11: ankle_left 12: foot_left 13: hip_right 14: knee_right 15: ankle_right 16: foot_right property-like structure : the available properties are the following: type: string containing skeleton's type (\"assistive_rehab::SkeletonStd\"). tag: string containing skeleton's tag. transformation: 4 x 4 skeleton's roto-translation matrix. coronal: vector containing skeleton's coronal plane. sagittal: vector containing skeleton's sagittal plane. transverse: vector containing skeleton's transverse plane. skeleton: list containing keypoints with the following subproperties: tag: string containing keypoint's tag. status: string containing keypoint's status (updated or stale). position: vector containing keypoint's camera coordinates x,y,z. pixel: vector containing keypoint's image coordinates u,v. child: list containing keypoint's child, specified as position, status, tag. An example is the following: (coronal (-0.0171187190241832 0.168776145353523 -0.984581522687332)) (sagittal (0.999852451902468 0.0040979367701588 -0.0166817666585492)) (skeleton (((child (((pixel (143.0 18.0)) (position (-0.0958624515848492 -0.655793450456223 2.01140785217285)) (status updated) (tag head)) ((child (((child (((pixel (182.0 106.0)) (position (0.132475296984726 -0.0970469850595488 2.24838447570801)) (status updated) (tag handLeft)))) (pixel (179.0 77.0)) (position (0.126047049721491 -0.301864959631551 2.20449256896973)) (status updated) (tag elbowLeft)))) (pixel (173.0 40.0)) (position (0.0829495294433831 -0.549497736338085 2.17046546936035)) (status updated) (tag shoulderLeft)) ((child (((child (((pixel (117.0 102.0)) (position (-0.318757589652556 -0.129758383247854 2.30834770202637)) (status updated) (tag handRight)))) (pixel (124.0 75.0)) (position (-0.265819393982889 -0.319631634579399 2.22591972351074)) (status updated) (tag elbowRight)))) (pixel (132.0 42.0)) (position (-0.194277581569445 -0.550633963158095 2.17509078979492)) (status updated) (tag shoulderRight)) ((child (((child (((child (((child (((pixel (153.0 200.0)) (position (-0.0305329581785242 0.615750619982796 2.40194511413574)) (status updated) (tag footLeft)))) (pixel (156.0 187.0)) (position (-0.0161527965668162 0.538963623868779 2.52594947814941)) (status updated) (tag ankleLeft)))) (pixel (158.0 148.0)) (position (-0.00361483866030924 0.218990793550177 2.42797660827637)) (status updated) (tag kneeLeft)))) (pixel (164.0 105.0)) (position (0.0249104292284522 -0.102991968370345 2.2304573059082)) (status updated) (tag hipLeft)) ((child (((child (((child (((pixel (138.0 194.0)) (position (-0.170998545149134 0.592533010093213 2.49751472473145)) (status updated) (tag footRight)))) (pixel (144.0 182.0)) (position (-0.130142124436922 0.509498389750101 2.55434036254883)) (status updated) (tag ankleRight)))) (pixel (142.0 147.0)) (position (-0.133305882322194 0.209480672220591 2.39202308654785)) (status updated) (tag kneeRight)))) (pixel (138.0 103.0)) (position (-0.158599348153779 -0.109193487775799 2.21346664428711)) (status updated) (tag hipRight)))) (pixel (151.0 103.0)) (position (-0.0738603465810386 -0.102184160595443 2.20890045166016)) (status updated) (tag hipCenter)))) (pixel (153.0 41.0)) (position (-0.054339762871519 -0.540136057902244 2.13348770141602)) (status updated) (tag shoulderCenter)))) (tag \"#7\") (transformation (4 4 (1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0))) (transverse (0.0438836580749896 -0.984546958261919 -0.169533216601232)) (type \"assistive_rehab::SkeletonStd\") From an unordered list of keypoints \u00b6 The following code snippet creates a SkeletonStd object from an unordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < std :: pair < std :: string , yarp :: sig :: Vector >> unordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: head , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_right , p )); } skeleton . setTag ( \"unordered\" ); skeleton . update ( unordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"unordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 1.000) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.000 -1.000 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = ( 0.000 0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} Note Don't get worried about nan in the pixel fields: it's correct since we didn't set them up yet. Uninitialized points and/or pixels take nan values. From an ordered list of keypoints \u00b6 The following code snippet creates a SkeletonStd object from an ordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < yarp :: sig :: Vector > ordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } skeleton . setTag ( \"ordered\" ); skeleton . update ( ordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"ordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 0.707) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.707 -0.707 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} From a property-like structure \u00b6 Assuming that the object skeleton1 has been previously defined using one of the instructions above, a new Skeleton object can be defined as following: yarp :: os :: Property prop = skeleton1 . toProperty (); std :: unique_ptr < assistive_rehab :: Skeleton > skeleton2 ( assistive_rehab :: skeleton_factory ( prop )); skeleton2 -> setTag ( \"properties\" ); skeleton2 -> print (); Retrieving a skeleton \u00b6 A skeleton can be retrieved as a property-like structure from a yarp port: assistive_rehab :: SkeletonStd skeleton ; if ( yarp :: os :: Bottle * b = opcPort . read ( false )) { yarp :: os :: Property prop ; prop . fromString ( b -> get ( 0 ). asList () -> toString ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } or from the OPC as following: assistive_rehab :: SkeletonStd skeleton ; yarp :: os :: Bottle cmd , reply ; cmd . addVocab ( yarp :: os :: Vocab :: encode ( \"ask\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); content . addString ( \"skeleton\" ); opcPort . write ( cmd , reply ); if ( reply . size () > 1 ) { if ( reply . get ( 0 ). asVocab () == Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * idField = reply . get ( 1 ). asList ()) { if ( yarp :: os :: Bottle * idValues = idField -> get ( 1 ). asList ()) { int id = idValues -> get ( 0 ). asInt (); cmd . clear (); cmd . addVocab ( Vocab :: encode ( \"get\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); yarp :: os :: Bottle replyProp ; content . addString ( \"id\" ); content . addInt ( id ); opcPort . write ( cmd , replyProp ); if ( replyProp . get ( 0 ). asVocab () == yarp :: os :: Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * propField = replyProp . get ( 1 ). asList ()) { yarp :: os :: Property prop ( propField -> toString (). c_str ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } } } } } } Accessing single keypoints of a skeleton \u00b6 If you want to access to a keypoint of the skeleton, you can use the operator[] of the class Skeleton , by passing as parameter the keypoint's tag. For example, the following snippet allows you to get the shoulder_center 3D coordinates: yarp :: sig :: Vector sc = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); Dealing with pixels \u00b6 Occasionally, it might be worth storing also the pixels alongside the points, which are used by the algorithm to reconstruct the 3D skeleton. This is particularly useful when the skeleton is employed to enable gaze tracking, for example. In this context, the 3D information of the keypoints needs normally to be transformed from the camera frame to the root frame of the robot. Instead, having the pixels would ease this process. The updating methods described above do have their pixel-wise counterparts: std :: vector < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >> ordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); ordered_withpixels . push_back ( std :: make_pair ( p , px )); skeleton . update_withpixels ( ordered_withpixels ); std :: vector < std :: pair < std :: string < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >>> unordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); unordered_withpixels . push_back ( std :: make_pair ( assistive_rehab :: KeyPointTag :: shoulder_center , std :: make_pair ( p , px ))); skeleton . update_withpixels ( unordered_withpixels ); yarp :: sig :: Vector pixel = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ]. getPixel (); Additional functionalities \u00b6 Normalization \u00b6 Some applications might require a normalized skeleton, to avoid having different results for different human physiques. The normalization provided in the library makes the length of the observed human segments always equal to 1 and can be applied as following: skeleton . normalize (); Keypoints' reference system \u00b6 Keypoints in the skeleton are defined with respect to the camera. To change the reference system, given a transformation matrix T , you can use the method setTransformation , as following: skeleton . setTransformation ( T ); Tip If you want to use the skeleton as reference system, you can create the transformation matrix T from the skeleton's planes: yarp :: sig :: Vector coronal = skeleton . getCoronal (); yarp :: sig :: Vector sagittal = skeleton . getSagittal (); yarp :: sig :: Vector transverse = skeleton . getTransverse (); yarp :: sig :: Vector p = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); yarp :: sig :: Matrix T1 ( 4 , 4 ); T1 . setSubcol ( coronal , 0 , 0 ); T1 . setSubcol ( sagittal , 0 , 1 ); T1 . setSubcol ( transverse , 0 , 2 ); T1 . setSubcol ( p , 0 , 3 ); T1 ( 3 , 3 ) = 1.0 ; T = yarp :: math :: SE3inv ( T1 ); skeleton . setTransformation ( T );","title":"How to manage a skeleton object"},{"location":"create_new_skeleton/#how-to-manage-a-skeleton-object","text":"This tutorial will explain you the main functionalities to manage a skeleton object for: defining a skeleton; retrieving a skeleton; accessing single keypoints of a skeleton; dealing with pixels; additional functionalities.","title":"How to manage a skeleton object"},{"location":"create_new_skeleton/#defining-a-skeleton","text":"A skeleton can be defined as: unordered list of keypoints : the list defines the keypoints as pairs of points associated to the keypoint tag; ordered list of keypoints : the list defines the keypoints according to the following order: 0: shoulder_center 1: head 2: shoulder_left 3: elbow_left 4: hand_left 5: shoulder_right 6: elbow_right 7: hand_right 8: hip_center 9: hip_left 10: knee_left 11: ankle_left 12: foot_left 13: hip_right 14: knee_right 15: ankle_right 16: foot_right property-like structure : the available properties are the following: type: string containing skeleton's type (\"assistive_rehab::SkeletonStd\"). tag: string containing skeleton's tag. transformation: 4 x 4 skeleton's roto-translation matrix. coronal: vector containing skeleton's coronal plane. sagittal: vector containing skeleton's sagittal plane. transverse: vector containing skeleton's transverse plane. skeleton: list containing keypoints with the following subproperties: tag: string containing keypoint's tag. status: string containing keypoint's status (updated or stale). position: vector containing keypoint's camera coordinates x,y,z. pixel: vector containing keypoint's image coordinates u,v. child: list containing keypoint's child, specified as position, status, tag. An example is the following: (coronal (-0.0171187190241832 0.168776145353523 -0.984581522687332)) (sagittal (0.999852451902468 0.0040979367701588 -0.0166817666585492)) (skeleton (((child (((pixel (143.0 18.0)) (position (-0.0958624515848492 -0.655793450456223 2.01140785217285)) (status updated) (tag head)) ((child (((child (((pixel (182.0 106.0)) (position (0.132475296984726 -0.0970469850595488 2.24838447570801)) (status updated) (tag handLeft)))) (pixel (179.0 77.0)) (position (0.126047049721491 -0.301864959631551 2.20449256896973)) (status updated) (tag elbowLeft)))) (pixel (173.0 40.0)) (position (0.0829495294433831 -0.549497736338085 2.17046546936035)) (status updated) (tag shoulderLeft)) ((child (((child (((pixel (117.0 102.0)) (position (-0.318757589652556 -0.129758383247854 2.30834770202637)) (status updated) (tag handRight)))) (pixel (124.0 75.0)) (position (-0.265819393982889 -0.319631634579399 2.22591972351074)) (status updated) (tag elbowRight)))) (pixel (132.0 42.0)) (position (-0.194277581569445 -0.550633963158095 2.17509078979492)) (status updated) (tag shoulderRight)) ((child (((child (((child (((child (((pixel (153.0 200.0)) (position (-0.0305329581785242 0.615750619982796 2.40194511413574)) (status updated) (tag footLeft)))) (pixel (156.0 187.0)) (position (-0.0161527965668162 0.538963623868779 2.52594947814941)) (status updated) (tag ankleLeft)))) (pixel (158.0 148.0)) (position (-0.00361483866030924 0.218990793550177 2.42797660827637)) (status updated) (tag kneeLeft)))) (pixel (164.0 105.0)) (position (0.0249104292284522 -0.102991968370345 2.2304573059082)) (status updated) (tag hipLeft)) ((child (((child (((child (((pixel (138.0 194.0)) (position (-0.170998545149134 0.592533010093213 2.49751472473145)) (status updated) (tag footRight)))) (pixel (144.0 182.0)) (position (-0.130142124436922 0.509498389750101 2.55434036254883)) (status updated) (tag ankleRight)))) (pixel (142.0 147.0)) (position (-0.133305882322194 0.209480672220591 2.39202308654785)) (status updated) (tag kneeRight)))) (pixel (138.0 103.0)) (position (-0.158599348153779 -0.109193487775799 2.21346664428711)) (status updated) (tag hipRight)))) (pixel (151.0 103.0)) (position (-0.0738603465810386 -0.102184160595443 2.20890045166016)) (status updated) (tag hipCenter)))) (pixel (153.0 41.0)) (position (-0.054339762871519 -0.540136057902244 2.13348770141602)) (status updated) (tag shoulderCenter)))) (tag \"#7\") (transformation (4 4 (1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0))) (transverse (0.0438836580749896 -0.984546958261919 -0.169533216601232)) (type \"assistive_rehab::SkeletonStd\")","title":"Defining a skeleton"},{"location":"create_new_skeleton/#from-an-unordered-list-of-keypoints","text":"The following code snippet creates a SkeletonStd object from an unordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < std :: pair < std :: string , yarp :: sig :: Vector >> unordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: head , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_right , p )); } skeleton . setTag ( \"unordered\" ); skeleton . update ( unordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"unordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 1.000) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.000 -1.000 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = ( 0.000 0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} Note Don't get worried about nan in the pixel fields: it's correct since we didn't set them up yet. Uninitialized points and/or pixels take nan values.","title":"From an unordered list of keypoints"},{"location":"create_new_skeleton/#from-an-ordered-list-of-keypoints","text":"The following code snippet creates a SkeletonStd object from an ordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < yarp :: sig :: Vector > ordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } skeleton . setTag ( \"ordered\" ); skeleton . update ( ordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"ordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 0.707) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.707 -0.707 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={}","title":"From an ordered list of keypoints"},{"location":"create_new_skeleton/#from-a-property-like-structure","text":"Assuming that the object skeleton1 has been previously defined using one of the instructions above, a new Skeleton object can be defined as following: yarp :: os :: Property prop = skeleton1 . toProperty (); std :: unique_ptr < assistive_rehab :: Skeleton > skeleton2 ( assistive_rehab :: skeleton_factory ( prop )); skeleton2 -> setTag ( \"properties\" ); skeleton2 -> print ();","title":"From a property-like structure"},{"location":"create_new_skeleton/#retrieving-a-skeleton","text":"A skeleton can be retrieved as a property-like structure from a yarp port: assistive_rehab :: SkeletonStd skeleton ; if ( yarp :: os :: Bottle * b = opcPort . read ( false )) { yarp :: os :: Property prop ; prop . fromString ( b -> get ( 0 ). asList () -> toString ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } or from the OPC as following: assistive_rehab :: SkeletonStd skeleton ; yarp :: os :: Bottle cmd , reply ; cmd . addVocab ( yarp :: os :: Vocab :: encode ( \"ask\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); content . addString ( \"skeleton\" ); opcPort . write ( cmd , reply ); if ( reply . size () > 1 ) { if ( reply . get ( 0 ). asVocab () == Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * idField = reply . get ( 1 ). asList ()) { if ( yarp :: os :: Bottle * idValues = idField -> get ( 1 ). asList ()) { int id = idValues -> get ( 0 ). asInt (); cmd . clear (); cmd . addVocab ( Vocab :: encode ( \"get\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); yarp :: os :: Bottle replyProp ; content . addString ( \"id\" ); content . addInt ( id ); opcPort . write ( cmd , replyProp ); if ( replyProp . get ( 0 ). asVocab () == yarp :: os :: Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * propField = replyProp . get ( 1 ). asList ()) { yarp :: os :: Property prop ( propField -> toString (). c_str ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } } } } } }","title":"Retrieving a skeleton"},{"location":"create_new_skeleton/#accessing-single-keypoints-of-a-skeleton","text":"If you want to access to a keypoint of the skeleton, you can use the operator[] of the class Skeleton , by passing as parameter the keypoint's tag. For example, the following snippet allows you to get the shoulder_center 3D coordinates: yarp :: sig :: Vector sc = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint ();","title":"Accessing single keypoints of a skeleton"},{"location":"create_new_skeleton/#dealing-with-pixels","text":"Occasionally, it might be worth storing also the pixels alongside the points, which are used by the algorithm to reconstruct the 3D skeleton. This is particularly useful when the skeleton is employed to enable gaze tracking, for example. In this context, the 3D information of the keypoints needs normally to be transformed from the camera frame to the root frame of the robot. Instead, having the pixels would ease this process. The updating methods described above do have their pixel-wise counterparts: std :: vector < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >> ordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); ordered_withpixels . push_back ( std :: make_pair ( p , px )); skeleton . update_withpixels ( ordered_withpixels ); std :: vector < std :: pair < std :: string < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >>> unordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); unordered_withpixels . push_back ( std :: make_pair ( assistive_rehab :: KeyPointTag :: shoulder_center , std :: make_pair ( p , px ))); skeleton . update_withpixels ( unordered_withpixels ); yarp :: sig :: Vector pixel = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ]. getPixel ();","title":"Dealing with pixels"},{"location":"create_new_skeleton/#additional-functionalities","text":"","title":"Additional functionalities"},{"location":"create_new_skeleton/#normalization","text":"Some applications might require a normalized skeleton, to avoid having different results for different human physiques. The normalization provided in the library makes the length of the observed human segments always equal to 1 and can be applied as following: skeleton . normalize ();","title":"Normalization"},{"location":"create_new_skeleton/#keypoints-reference-system","text":"Keypoints in the skeleton are defined with respect to the camera. To change the reference system, given a transformation matrix T , you can use the method setTransformation , as following: skeleton . setTransformation ( T ); Tip If you want to use the skeleton as reference system, you can create the transformation matrix T from the skeleton's planes: yarp :: sig :: Vector coronal = skeleton . getCoronal (); yarp :: sig :: Vector sagittal = skeleton . getSagittal (); yarp :: sig :: Vector transverse = skeleton . getTransverse (); yarp :: sig :: Vector p = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); yarp :: sig :: Matrix T1 ( 4 , 4 ); T1 . setSubcol ( coronal , 0 , 0 ); T1 . setSubcol ( sagittal , 0 , 1 ); T1 . setSubcol ( transverse , 0 , 2 ); T1 . setSubcol ( p , 0 , 3 ); T1 ( 3 , 3 ) = 1.0 ; T = yarp :: math :: SE3inv ( T1 ); skeleton . setTransformation ( T );","title":"Keypoints' reference system"},{"location":"disembodied/","text":"How to run the visual pipeline in a disembodied manner \u00b6 The visual pipeline can be run also without a physical robot, i.e. with a (disembodied) realsense camera . This tutorial will show you how to do it. Warning The visual pipeline relies on yarpOpenPose and actionRecognizer , which require an NVIDIA graphics card to be run. First, you need to run yarpserver . You can open a terminal and type: yarpserver Connect the realsense to your laptop. Seealso If you have not done it, you will need to install it. Warning yarp has to be compiled with ENABLE_yarpmod_realsense2 ON . Open yarpmanager , run the Assistive_Rehab_App and hit connect . Note All the modules that require a robot must not be run, i.e. faceDisplayServer , faceExpressionImage , attentionManager , cer_gaze-controller , interactionManager , ctpService , cer_reaching-solver and cer_reaching-controller . Tip You can customize the app by removing unnecessary modules and replacing the nodes in the xml with localhost. You can save the app in the folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , the app will be automatically loaded. Once all the modules are running, you need to send commands to motionAnalyzer , in order to select the metric and the skeleton that you want to analyze. Therefore you can open a terminal and type: yarp rpc /motionAnalyzer/cmd loadMetric metric_tag selectSkel skeleton_tag start where metric_tag and skeleton_tag are respectively the tag of the metric and the tag of the skeleton under analysis (for example ROM_0 and #0 ). Tip The available metrics can be listed using the command listMetrics . When the command start is given to motionAnalyzer , the visual pipeline starts, i.e. the template skeleton is loaded and visualized on skeletonViewer , the extracted metric is visualized on yarpscope and the action recognition pipeline starts. You should have the following situation: A verbal feedback is also provided throughout the experiment. When you want to stop the pipeline, you need to send a stop command to motionAnalyzer , by typing stop in the previous terminal.","title":"How to run the pipeline in a disembodied manner"},{"location":"disembodied/#how-to-run-the-visual-pipeline-in-a-disembodied-manner","text":"The visual pipeline can be run also without a physical robot, i.e. with a (disembodied) realsense camera . This tutorial will show you how to do it. Warning The visual pipeline relies on yarpOpenPose and actionRecognizer , which require an NVIDIA graphics card to be run. First, you need to run yarpserver . You can open a terminal and type: yarpserver Connect the realsense to your laptop. Seealso If you have not done it, you will need to install it. Warning yarp has to be compiled with ENABLE_yarpmod_realsense2 ON . Open yarpmanager , run the Assistive_Rehab_App and hit connect . Note All the modules that require a robot must not be run, i.e. faceDisplayServer , faceExpressionImage , attentionManager , cer_gaze-controller , interactionManager , ctpService , cer_reaching-solver and cer_reaching-controller . Tip You can customize the app by removing unnecessary modules and replacing the nodes in the xml with localhost. You can save the app in the folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , the app will be automatically loaded. Once all the modules are running, you need to send commands to motionAnalyzer , in order to select the metric and the skeleton that you want to analyze. Therefore you can open a terminal and type: yarp rpc /motionAnalyzer/cmd loadMetric metric_tag selectSkel skeleton_tag start where metric_tag and skeleton_tag are respectively the tag of the metric and the tag of the skeleton under analysis (for example ROM_0 and #0 ). Tip The available metrics can be listed using the command listMetrics . When the command start is given to motionAnalyzer , the visual pipeline starts, i.e. the template skeleton is loaded and visualized on skeletonViewer , the extracted metric is visualized on yarpscope and the action recognition pipeline starts. You should have the following situation: A verbal feedback is also provided throughout the experiment. When you want to stop the pipeline, you need to send a stop command to motionAnalyzer , by typing stop in the previous terminal.","title":"How to run the visual pipeline in a disembodied manner"},{"location":"gazebo_plugin/","text":"How to use the gazebo plugin \u00b6 This tutorial will show you how to use the gazebo plugin we developed for an animated model, which is called actor in gazebo . As shown in this gazebo tutorial , an actor in gazebo is a model which can be animated using .dae files and scripted trajectories . Our plugin extends the actor class, allowing the user to play and stop single animations or the whole script, to change the speed on the fly, to reach targets in the environment. Dependencies \u00b6 After installing assistive-rehab , you will need the following dependencies: gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ; Preparing your environment \u00b6 The first step you need to take is to prepare your environment. Installing assistive-rehab will result in the shared library libgazebo_assistiverehab_tuginterface.so , which can be included in a gazebo simulation. Following this gazebo tutorial, you need to let gazebo know where the plugin shared library is located. For doing this, you will need to set the following environment variables: GAZEBO_PLUGIN_PATH : has to point to the folder where you installed the shared library; GAZEBO_MODEL_PATH : has to point to the folder including the actor .dae files; gazebo models gazebo currently provides several .dae animations, which can be found here . GAZEBO_RESOURCE_PATH : has to point to the folder including your world application. How to include the plugin in your world \u00b6 To include the plugin in your world, you can add the highlighted lines to the actor tag: <actor name= \"actor\" > <skin> <filename> stand.dae </filename> </skin> <animation name= \"stand\" > <filename> stand.dae </filename> </animation> <animation name= \"sit_down\" > <filename> sit_down.dae </filename> </animation> <animation name= \"stand_up\" > <filename> stand_up.dae </filename> </animation> <animation name= \"walk\" > <filename> walk.dae </filename> <interpolate_x> true </interpolate_x> </animation> <script> <loop> false </loop> <auto_start> false </auto_start> <trajectory id= \"0\" type= \"stand\" /> <trajectory id= \"1\" type= \"sit_down\" /> <trajectory id= \"2\" type= \"stand_up\" /> <trajectory id= \"3\" type= \"walk\" /> </script> <plugin name= 'tug_interface' filename= 'libgazebo_assistiverehab_tuginterface.so' > <yarpConfigurationFile> model://tugInterface.ini </yarpConfigurationFile> </plugin> </actor> This example snippet generates an actor associated to three animations, stand.dae , sit_down.dae , stand_up.dae and walk.dae . The plugin loads the tugInterface.ini configuration file and opens a rpc port named by default /tug_input_port , which gives you access to a set of thrift services, described in the next section. Important To use the plugin you will need to have an actor with at least one animation associated. Note We are going to refer to this snippet in next section to see the plugin functionalities, but you can personalize the actor according to your needs. Using the plugin \u00b6 To start using the plugin, first open a terminal and run yarpserver . Open a new terminal and type gazebo followed by the name of your scenario. Remember gazebo looks for world files in the $GAZEBO_RESOURCE_PATH environment variable, as described at the beginning of this tutorial. Now you should see an actor standing on your screen: You can start playing with the plugin! Open a terminal and type yarp rpc /tug_input_port . Playing the animations \u00b6 Type play on the terminal. All the animations are going to be played in a row, following the sequence defined by the ids in the trajectories as defined here : Additionally, you can play the single animations associated to the actor or play the whole script from the specified animation: PLAY SINGLE ANIMATION PLAY FROM ANIMATION play stand play stand -1 true play sit_down play sit_down -1 true play stand_up play stand_up -1 true play walk play walk -1 true Retrieving animations You can retrieve the list of animations associated to your actor using the rpc command getAnimationList . Example Playing a single animation of the script is useful when you need a condition to be accomplished before the specific animation is played. For example, in the TUG scenario, we want the actor to sit or to start walking only when the robot gives the related command. You can check it out here . Changing walking parameters \u00b6 The path followed during the walk animation is specified by targets in the tugInterface.ini configuration file: this is a matrix containing the poses to reach in the form x y theta . Reference frame The targets are defined with respect to the gazebo world frame, thus X pointing forward (with respect to the human model), Y pointing left, Z pointing up. For our application , we need the human model to reach the target and go back to the initial position. You can control the trajectory through the following rpc commands: DESCRIPTION OUTPUT setTarget x y theta : providing x y theta , when playing the walk animation, the human model will reach the new specified target and go back. - setTarget 3.5 2.0 10.0 - play walk goToSeq (x1 y1 theta1 ... xN yN thetaN) : providing the list of xi yi thetai , the human model reaches the specified waypoints. - goToSeq (3.0 2.0 20.0 5.0 0.0 -20.0) Note The goToSeq is a blocking service: only when the last target is reached, an ack is returned. Tip The interface provides two additional services, goTo and goToWait which are the punctual versions of goToSeq , respectively non blocking and blocking. Finally, you can set the speed of the walking animation through the command setSpeed : WALKING SLOWER WALKING FASTER - setSpeed 0.5 - play walk - setSpeed 2.0 - play walk Additional services \u00b6 The additional services provided are the following: playFromLast : to play the animation from the last stop. A stop can be provided while an animation is being played. With this command, you will be able to start the script exactly from where you stopped. Note The whole script is played by default. The command playFromLast false plays only the animation coming after last stop. getState : to know the current animation being played; pause : to pause the actor for a specified time or for an unlimited time (if not specified and until the play command is provided).","title":"How to use the gazebo plugin"},{"location":"gazebo_plugin/#how-to-use-the-gazebo-plugin","text":"This tutorial will show you how to use the gazebo plugin we developed for an animated model, which is called actor in gazebo . As shown in this gazebo tutorial , an actor in gazebo is a model which can be animated using .dae files and scripted trajectories . Our plugin extends the actor class, allowing the user to play and stop single animations or the whole script, to change the speed on the fly, to reach targets in the environment.","title":"How to use the gazebo plugin"},{"location":"gazebo_plugin/#dependencies","text":"After installing assistive-rehab , you will need the following dependencies: gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ;","title":"Dependencies"},{"location":"gazebo_plugin/#preparing-your-environment","text":"The first step you need to take is to prepare your environment. Installing assistive-rehab will result in the shared library libgazebo_assistiverehab_tuginterface.so , which can be included in a gazebo simulation. Following this gazebo tutorial, you need to let gazebo know where the plugin shared library is located. For doing this, you will need to set the following environment variables: GAZEBO_PLUGIN_PATH : has to point to the folder where you installed the shared library; GAZEBO_MODEL_PATH : has to point to the folder including the actor .dae files; gazebo models gazebo currently provides several .dae animations, which can be found here . GAZEBO_RESOURCE_PATH : has to point to the folder including your world application.","title":"Preparing your environment"},{"location":"gazebo_plugin/#how-to-include-the-plugin-in-your-world","text":"To include the plugin in your world, you can add the highlighted lines to the actor tag: <actor name= \"actor\" > <skin> <filename> stand.dae </filename> </skin> <animation name= \"stand\" > <filename> stand.dae </filename> </animation> <animation name= \"sit_down\" > <filename> sit_down.dae </filename> </animation> <animation name= \"stand_up\" > <filename> stand_up.dae </filename> </animation> <animation name= \"walk\" > <filename> walk.dae </filename> <interpolate_x> true </interpolate_x> </animation> <script> <loop> false </loop> <auto_start> false </auto_start> <trajectory id= \"0\" type= \"stand\" /> <trajectory id= \"1\" type= \"sit_down\" /> <trajectory id= \"2\" type= \"stand_up\" /> <trajectory id= \"3\" type= \"walk\" /> </script> <plugin name= 'tug_interface' filename= 'libgazebo_assistiverehab_tuginterface.so' > <yarpConfigurationFile> model://tugInterface.ini </yarpConfigurationFile> </plugin> </actor> This example snippet generates an actor associated to three animations, stand.dae , sit_down.dae , stand_up.dae and walk.dae . The plugin loads the tugInterface.ini configuration file and opens a rpc port named by default /tug_input_port , which gives you access to a set of thrift services, described in the next section. Important To use the plugin you will need to have an actor with at least one animation associated. Note We are going to refer to this snippet in next section to see the plugin functionalities, but you can personalize the actor according to your needs.","title":"How to include the plugin in your world"},{"location":"gazebo_plugin/#using-the-plugin","text":"To start using the plugin, first open a terminal and run yarpserver . Open a new terminal and type gazebo followed by the name of your scenario. Remember gazebo looks for world files in the $GAZEBO_RESOURCE_PATH environment variable, as described at the beginning of this tutorial. Now you should see an actor standing on your screen: You can start playing with the plugin! Open a terminal and type yarp rpc /tug_input_port .","title":"Using the plugin"},{"location":"gazebo_plugin/#playing-the-animations","text":"Type play on the terminal. All the animations are going to be played in a row, following the sequence defined by the ids in the trajectories as defined here : Additionally, you can play the single animations associated to the actor or play the whole script from the specified animation: PLAY SINGLE ANIMATION PLAY FROM ANIMATION play stand play stand -1 true play sit_down play sit_down -1 true play stand_up play stand_up -1 true play walk play walk -1 true Retrieving animations You can retrieve the list of animations associated to your actor using the rpc command getAnimationList . Example Playing a single animation of the script is useful when you need a condition to be accomplished before the specific animation is played. For example, in the TUG scenario, we want the actor to sit or to start walking only when the robot gives the related command. You can check it out here .","title":"Playing the animations"},{"location":"gazebo_plugin/#changing-walking-parameters","text":"The path followed during the walk animation is specified by targets in the tugInterface.ini configuration file: this is a matrix containing the poses to reach in the form x y theta . Reference frame The targets are defined with respect to the gazebo world frame, thus X pointing forward (with respect to the human model), Y pointing left, Z pointing up. For our application , we need the human model to reach the target and go back to the initial position. You can control the trajectory through the following rpc commands: DESCRIPTION OUTPUT setTarget x y theta : providing x y theta , when playing the walk animation, the human model will reach the new specified target and go back. - setTarget 3.5 2.0 10.0 - play walk goToSeq (x1 y1 theta1 ... xN yN thetaN) : providing the list of xi yi thetai , the human model reaches the specified waypoints. - goToSeq (3.0 2.0 20.0 5.0 0.0 -20.0) Note The goToSeq is a blocking service: only when the last target is reached, an ack is returned. Tip The interface provides two additional services, goTo and goToWait which are the punctual versions of goToSeq , respectively non blocking and blocking. Finally, you can set the speed of the walking animation through the command setSpeed : WALKING SLOWER WALKING FASTER - setSpeed 0.5 - play walk - setSpeed 2.0 - play walk","title":"Changing walking parameters"},{"location":"gazebo_plugin/#additional-services","text":"The additional services provided are the following: playFromLast : to play the animation from the last stop. A stop can be provided while an animation is being played. With this command, you will be able to start the script exactly from where you stopped. Note The whole script is played by default. The command playFromLast false plays only the animation coming after last stop. getState : to know the current animation being played; pause : to pause the actor for a specified time or for an unlimited time (if not specified and until the play command is provided).","title":"Additional services"},{"location":"install/","text":"Install \u00b6 Disclaimer Assistive-rehab has been widely tested on Ubuntu 16.04 and Ubuntu 18.04 . If you face any issue either with your OS, please submit an Issue . Requirements \u00b6 Supported Operating Systems: Linux, Windows, macOS C++11 compiler CMake 3.5 ycm icub-contrib-common yarp (3.1.100 or higher) iCub OpenCV (3.4.0 or higher) yarpOpenPose Ipopt yarp.js yarp ENABLE_yarpcar_mjpeg ON : to allow mjpeg compression. ENABLE_yarpcar_zfp ON : to allow zfp compression. ENABLE_yarpmod_realsense2 ON : to enable the realsense. OpenCV Download OpenCV: git clone https://github.com/opencv/opencv.git . Checkout the correct branch/tag: git checkout 3.4.0 . Download the external modules: git clone https://github.com/opencv/opencv_contrib.git . Checkout the same branch/tag: git checkout 3.4.0 . Configure OpenCV by filling in OPENCV_EXTRA_MODULES_PATH with the path to opencv_contrib/modules and then toggling on all possible modules. Compile OpenCV. Optional dependencies \u00b6 Dependency License TensorFlowCC MIT fftw3 GPL GSL GPL matio BSD 2-Clause VTK (8.1.0 or higher) BSD-style cer GPL TensorFlowCC TensorFlowCC builds and installs the TensorFlow C++ API, which is released under Apache 2.0 license. matio On Ubuntu 18.04 , you can install the library through apt: sudo apt install libmatio-dev . Warning If an optional dependency is not found, the modules depending on it are not compiled. Failure If you want to run the full demo, also additional dependencies are required. report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly warnings collections math re os datetime IPython plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Installation \u00b6 If all the dependencies are met, proceed with the following instructions: From sources Substitute to <install-prefix> the absolute path where you want to install the project. GNU/Linux and macOS git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install Windows git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install","title":"Install"},{"location":"install/#install","text":"Disclaimer Assistive-rehab has been widely tested on Ubuntu 16.04 and Ubuntu 18.04 . If you face any issue either with your OS, please submit an Issue .","title":"Install"},{"location":"install/#requirements","text":"Supported Operating Systems: Linux, Windows, macOS C++11 compiler CMake 3.5 ycm icub-contrib-common yarp (3.1.100 or higher) iCub OpenCV (3.4.0 or higher) yarpOpenPose Ipopt yarp.js yarp ENABLE_yarpcar_mjpeg ON : to allow mjpeg compression. ENABLE_yarpcar_zfp ON : to allow zfp compression. ENABLE_yarpmod_realsense2 ON : to enable the realsense. OpenCV Download OpenCV: git clone https://github.com/opencv/opencv.git . Checkout the correct branch/tag: git checkout 3.4.0 . Download the external modules: git clone https://github.com/opencv/opencv_contrib.git . Checkout the same branch/tag: git checkout 3.4.0 . Configure OpenCV by filling in OPENCV_EXTRA_MODULES_PATH with the path to opencv_contrib/modules and then toggling on all possible modules. Compile OpenCV.","title":"Requirements"},{"location":"install/#optional-dependencies","text":"Dependency License TensorFlowCC MIT fftw3 GPL GSL GPL matio BSD 2-Clause VTK (8.1.0 or higher) BSD-style cer GPL TensorFlowCC TensorFlowCC builds and installs the TensorFlow C++ API, which is released under Apache 2.0 license. matio On Ubuntu 18.04 , you can install the library through apt: sudo apt install libmatio-dev . Warning If an optional dependency is not found, the modules depending on it are not compiled. Failure If you want to run the full demo, also additional dependencies are required. report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly warnings collections math re os datetime IPython plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user .","title":"Optional dependencies"},{"location":"install/#installation","text":"If all the dependencies are met, proceed with the following instructions: From sources Substitute to <install-prefix> the absolute path where you want to install the project. GNU/Linux and macOS git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install Windows git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install","title":"Installation"},{"location":"latest_news/","text":"Latest news \u00b6 May 22, 2020 : Checkout our latest release v0.5.0 ! What's new? The clinical test Timed Up and Go ( TUG ) is now ready for both the real robot and within the simulation environment gazebo : Follow the tutorial to run the demo in gazebo ! Tip Click on the image to open the video! The motion analysis has been extended to the lower limbs: now we can evaluate walking parameters, such as step length and width, walking speed and number of steps. We developed the lineDetector module to visually detect start and finish lines on the floor, composed of ArUco markers. We developed a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path. Note The environment is supposed to be free from obstacles. We integrated the Google services API within our application to have a simple natural question and answer mechanism: the googleSpeech module receives the sound from a microphone and retrieves the speech transcript from Google Speech cloud services; the googleSpeechProcess module receives the speech transcript and analyses the sentence to retrieve its structure and meaning, relying on Google Language Cloud services. The speech system can be triggered by a Mystrom wifi button , which avoids the system to be always listening and thus responsive also to background noise: whenever the user presses the button, the robot is ready to answer questions in italian ! Note The robot can answer a selected set of questions related to the TUG. The interaction is still flexible as questions can be posed by the user in natural language, thanks to the capability of the system to interpret the question, rather than simply recognize it. March 19, 2020 : Added new tutorial to detect Aruco boards in gazebo . Check it out! July 10, 2019 : Checkout our latest release v0.4.0 ! What's new? the feedback can be provided now using the robot skeleton template , rather than the pre-recorded one. The new module robotSkeletonPublisher publishes the robot skeleton, which represents R1 limbs configuration, as following: The robot skeleton is remapped onto the observed skeleton internally within feedbackProducer for the further analysis ( skeletonScaler and skeletonPlayer are thus bypassed). Such modality insures a full synchronization between the robot movement and the skeleton template, which was not guaranteed with the pre-recorded template. Note The modality with the pre-recorded template is still available and can be set through interactionManager by setting the flag use-robot-template to false . In such case, the pipeline including skeletonScaler and skeletonPlayer is used. Tip The robot skeleton can be replayed offline by saving the robot joints specified in this app . A tutorial for replaying a full experiment can be found in the Tutorial section. the Train With Me study aims at comparing users' engagement during a physical training session with a real robot and a virtual agent. Preliminary experiments were designed for comparing R1 with its virtual counterpart and the developed infrastructure is now available. interactionManager can deal with the following three phases: observation : the real/virtual robot shows the exercise and the user observes it; imitation : the real/virtual robot performs the exercise and the user imitates it, occlusion : the real/virtual robot keeps performing the exercise behind a panel and the user keeps imitating it, without having any feedback. The scripts used during experiments can be found here , namely AssistiveRehab-TWM-robot.xml.template and AssistiveRehab-TWM-virtual.xml.template , which load parameters defined in the train-with-me context. A tutorial for running the demo with the virtual R1 can be found in the Tutorial section. May 6, 2019 : Checkout our latest release v0.3.0 ! This is a major change which refactors the entire framework to deal also with feet, following up the use of BODY_25 model of OpenPose . The following is an example of skeleton with feet in 2D and 3D: Changes include: SkeletonStd now includes hip_center , foot_left , foot_right : hip_center is directly observed if available, otherwise is estimated as middle point between hip_left and hip_right (the same stands for shoulder_center ); foot_left and foot_right are detected as being the big-toe. If big-toe is not available, small-toe is used as fallback; SkeletonWaist has been removed in favor of the new SkeletonStd ; optimization performed by skeletonRetriever is now extended also to lower limbs; modules previously relying on SkeletonWaist have been updated to use the new SkeletonStd ; the new framework is compatible with the Y1M5 demo , which was successfully tested online on the robot; the new framework is compatible with datasets recorded before the release, which can be reproduced by means of yarpdataplayer . May 6, 2019 : Checkout our new release v0.2.1 ! What's new? the action recognition is now robust to rotations ! The original network was trained with skeletons frontal to the camera, which is not guaranteed during a real interaction. The network has been re-trained with a wider training set, comprising synthetic rotations applied to real data around each axis, with variation of 10 degrees in a range of [-20,20] degrees. Also a variability on the speed was introduced in the training set, by considering the action performed at normal, double and half speed. We compared the accuracy of the previous and the new model for different rotations of the skeleton, and results show a high accuracy to a wider range for all axes: ROTATION AROUND X Original New ROTATION AROUND Y Original New ROTATION AROUND Z Original New the skeleton now also stores the pixels alongside the 3D points! This is very useful when using the skeleton for gaze tracking, as it avoids the transformation from the camera frame to the root frame of the robot required is using 3D information; the offline report is now interactive ! The user can navigate through plots, zoom, pan, save: January 28, 2019 : Checkout our latest tutorial on how to run the main applications on the robot R1! January 25, 2019 : We are public now! December 21, 2018 : Checkout the latest release and the the comparison with the first release !","title":"Latest"},{"location":"latest_news/#latest-news","text":"May 22, 2020 : Checkout our latest release v0.5.0 ! What's new? The clinical test Timed Up and Go ( TUG ) is now ready for both the real robot and within the simulation environment gazebo : Follow the tutorial to run the demo in gazebo ! Tip Click on the image to open the video! The motion analysis has been extended to the lower limbs: now we can evaluate walking parameters, such as step length and width, walking speed and number of steps. We developed the lineDetector module to visually detect start and finish lines on the floor, composed of ArUco markers. We developed a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path. Note The environment is supposed to be free from obstacles. We integrated the Google services API within our application to have a simple natural question and answer mechanism: the googleSpeech module receives the sound from a microphone and retrieves the speech transcript from Google Speech cloud services; the googleSpeechProcess module receives the speech transcript and analyses the sentence to retrieve its structure and meaning, relying on Google Language Cloud services. The speech system can be triggered by a Mystrom wifi button , which avoids the system to be always listening and thus responsive also to background noise: whenever the user presses the button, the robot is ready to answer questions in italian ! Note The robot can answer a selected set of questions related to the TUG. The interaction is still flexible as questions can be posed by the user in natural language, thanks to the capability of the system to interpret the question, rather than simply recognize it. March 19, 2020 : Added new tutorial to detect Aruco boards in gazebo . Check it out! July 10, 2019 : Checkout our latest release v0.4.0 ! What's new? the feedback can be provided now using the robot skeleton template , rather than the pre-recorded one. The new module robotSkeletonPublisher publishes the robot skeleton, which represents R1 limbs configuration, as following: The robot skeleton is remapped onto the observed skeleton internally within feedbackProducer for the further analysis ( skeletonScaler and skeletonPlayer are thus bypassed). Such modality insures a full synchronization between the robot movement and the skeleton template, which was not guaranteed with the pre-recorded template. Note The modality with the pre-recorded template is still available and can be set through interactionManager by setting the flag use-robot-template to false . In such case, the pipeline including skeletonScaler and skeletonPlayer is used. Tip The robot skeleton can be replayed offline by saving the robot joints specified in this app . A tutorial for replaying a full experiment can be found in the Tutorial section. the Train With Me study aims at comparing users' engagement during a physical training session with a real robot and a virtual agent. Preliminary experiments were designed for comparing R1 with its virtual counterpart and the developed infrastructure is now available. interactionManager can deal with the following three phases: observation : the real/virtual robot shows the exercise and the user observes it; imitation : the real/virtual robot performs the exercise and the user imitates it, occlusion : the real/virtual robot keeps performing the exercise behind a panel and the user keeps imitating it, without having any feedback. The scripts used during experiments can be found here , namely AssistiveRehab-TWM-robot.xml.template and AssistiveRehab-TWM-virtual.xml.template , which load parameters defined in the train-with-me context. A tutorial for running the demo with the virtual R1 can be found in the Tutorial section. May 6, 2019 : Checkout our latest release v0.3.0 ! This is a major change which refactors the entire framework to deal also with feet, following up the use of BODY_25 model of OpenPose . The following is an example of skeleton with feet in 2D and 3D: Changes include: SkeletonStd now includes hip_center , foot_left , foot_right : hip_center is directly observed if available, otherwise is estimated as middle point between hip_left and hip_right (the same stands for shoulder_center ); foot_left and foot_right are detected as being the big-toe. If big-toe is not available, small-toe is used as fallback; SkeletonWaist has been removed in favor of the new SkeletonStd ; optimization performed by skeletonRetriever is now extended also to lower limbs; modules previously relying on SkeletonWaist have been updated to use the new SkeletonStd ; the new framework is compatible with the Y1M5 demo , which was successfully tested online on the robot; the new framework is compatible with datasets recorded before the release, which can be reproduced by means of yarpdataplayer . May 6, 2019 : Checkout our new release v0.2.1 ! What's new? the action recognition is now robust to rotations ! The original network was trained with skeletons frontal to the camera, which is not guaranteed during a real interaction. The network has been re-trained with a wider training set, comprising synthetic rotations applied to real data around each axis, with variation of 10 degrees in a range of [-20,20] degrees. Also a variability on the speed was introduced in the training set, by considering the action performed at normal, double and half speed. We compared the accuracy of the previous and the new model for different rotations of the skeleton, and results show a high accuracy to a wider range for all axes: ROTATION AROUND X Original New ROTATION AROUND Y Original New ROTATION AROUND Z Original New the skeleton now also stores the pixels alongside the 3D points! This is very useful when using the skeleton for gaze tracking, as it avoids the transformation from the camera frame to the root frame of the robot required is using 3D information; the offline report is now interactive ! The user can navigate through plots, zoom, pan, save: January 28, 2019 : Checkout our latest tutorial on how to run the main applications on the robot R1! January 25, 2019 : We are public now! December 21, 2018 : Checkout the latest release and the the comparison with the first release !","title":"Latest news"},{"location":"license/","text":"BSD 3-Clause \u00b6 BSD 3-Clause License Copyright \u00a9 2018, Robotology All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#bsd-3-clause","text":"BSD 3-Clause License Copyright \u00a9 2018, Robotology All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-Clause"},{"location":"main_apps/","text":"How to run the rehabilitation demos \u00b6 This tutorial will show you how to run the rehabilitation demos. The applications can be found here , namely AssistiveRehab.xml.template and AssistiveRehab-faces.xml.template . The basic app: AssistiveRehab.xml \u00b6 Let's start with the basic one: AssistiveRehab.xml.template . Tip AssistiveRehab-faces.xml.template builds upon AssistiveRehab.xml.template , introducing additional modules for face recognition. We assume you are working on the robot R1 and that r1-face , r1-torso1 , r1-cuda-linux , r1-console-linux , r1-base and r1-display-linux are available. R1 On R1, yarpserver runs on r1-base . R1-mk2 On R1-mk2, the cuda system is named r1-console-cuda , which we also use as display (therefore r1-display-linux is r1-console-cuda ). The interaction requires the robot's motors to be on. Therefore turn on the motors, then open a terminal and type: ssh r1-base cd $ROBOT_CODE/cer/app/robots/CER02 yarprobotinterface Now we are ready to run the application! Open a new terminal and type: yarpmanager Now click on Assistive_Rehab_App , hit run and then connect. The demo is now running! To make the robot look around and engage the user, just issue the start command to the interactionManager module. If the user accepts the invitation by lifting her/his hand, the interaction starts. The user has to repeat the exercise shown by the robot, which in turns evaluates how the exercise is being performed, through a verbal feedback. The session ends with the robot giving an overall feedback of the exercise. The interaction can go on, as the robot keeps memory of the past interaction. Let's look a bit deeper into the application to see which are the modules running: yarpdev --device speech --lingware-context speech --default-language it-IT --robot r1 --pitch 80 --speed 110 : to run the speech, by default in italian. You have to change the default-language to en-GB to switch to english; yarpdev --device faceDisplayServer : to run the face; yarpdev --device --context AssistiveRehab --from realsense2.ini : to run the camera; faceExpressionImage : to visualize the expressions and the talking mouth on the robot's display; iSpeak --package speech-dev : to make the robot speak; yarpOpenPose : to detect 2D skeletons; objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; attentionManager : to make the robot focus and follow a skeleton; cer_gaze-controller : to control the robot's gaze; motionAnalyzer --from motion-repertoire-rom12.ini : to analyze motion. This configuration file does not include the reaching exercise, which is under testing from the robot's point of view (the motion analysis is implemented); skeletonPlayer : to replay the template skeleton; skeletonScaler : to move/rescale the template skeleton; feedbackProducer : to produce a feedback depending on the exercise; actionRecognizer : to recognize the exercise being performed; feedbackSynthetizer : to generate a verbal feedback; yarpview : to visualize depth and RGB image with 2D skeletons; skeletonViewer : to visualize 3D skeletons; yarpscope : to visualize the metric on the movement in real-time; interactionManager : to supervise the interaction; ctpService (for each arm): to send commands to the robot's arms; cer_reaching-solver + cer_reaching-controller (for each arm): to let the robot performing a reaching task. When you want to stop the interaction, hit stop in yarpmanager . You can produce two final reports (in italian and english) of the performed exercises by opening a terminal and typing: assistive-rehab-generate-report.sh report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Two html files will be created in the folder where you run the script. Note Producing the report might take a while, as all the files whose names equal the name of the most recent file are processed. This allows us to create a report not only of the current session, but also of the clinical evolution of the patient. The basic app integrated with face recognition: AssistiveRehab-faces.xml \u00b6 The AssistiveRehab-faces.xml.template builds upon the basic app, introducing additional modules for face recognition, which are the following: R1-mk2 On R1-mk2, the face recognition pipeline is run on r1-torso2 , which has to be available. humanStructure : to restrict the area of recognition around human faces; caffeCoder : to extract a representation of the cropped image; linearClassifierModule : to classify the extracted features; faceRecognizer : to label human faces. For running the demo, click on Assistive_Rehab_with_Faces_App in yarpmanager , hit run and then connect. Note caffeCoder takes a while to load the network. Therefore, before hitting connect, wait some seconds. You can click the refresh icon and check when the caffeCoder ports become green. The demo is similar to the basic one, but now the robot interacts with people using their names! Tip If you want to train the network with your face to the face database, you can use the provided API .","title":"How to run the rehabilitation demos"},{"location":"main_apps/#how-to-run-the-rehabilitation-demos","text":"This tutorial will show you how to run the rehabilitation demos. The applications can be found here , namely AssistiveRehab.xml.template and AssistiveRehab-faces.xml.template .","title":"How to run the rehabilitation demos"},{"location":"main_apps/#the-basic-app-assistiverehabxml","text":"Let's start with the basic one: AssistiveRehab.xml.template . Tip AssistiveRehab-faces.xml.template builds upon AssistiveRehab.xml.template , introducing additional modules for face recognition. We assume you are working on the robot R1 and that r1-face , r1-torso1 , r1-cuda-linux , r1-console-linux , r1-base and r1-display-linux are available. R1 On R1, yarpserver runs on r1-base . R1-mk2 On R1-mk2, the cuda system is named r1-console-cuda , which we also use as display (therefore r1-display-linux is r1-console-cuda ). The interaction requires the robot's motors to be on. Therefore turn on the motors, then open a terminal and type: ssh r1-base cd $ROBOT_CODE/cer/app/robots/CER02 yarprobotinterface Now we are ready to run the application! Open a new terminal and type: yarpmanager Now click on Assistive_Rehab_App , hit run and then connect. The demo is now running! To make the robot look around and engage the user, just issue the start command to the interactionManager module. If the user accepts the invitation by lifting her/his hand, the interaction starts. The user has to repeat the exercise shown by the robot, which in turns evaluates how the exercise is being performed, through a verbal feedback. The session ends with the robot giving an overall feedback of the exercise. The interaction can go on, as the robot keeps memory of the past interaction. Let's look a bit deeper into the application to see which are the modules running: yarpdev --device speech --lingware-context speech --default-language it-IT --robot r1 --pitch 80 --speed 110 : to run the speech, by default in italian. You have to change the default-language to en-GB to switch to english; yarpdev --device faceDisplayServer : to run the face; yarpdev --device --context AssistiveRehab --from realsense2.ini : to run the camera; faceExpressionImage : to visualize the expressions and the talking mouth on the robot's display; iSpeak --package speech-dev : to make the robot speak; yarpOpenPose : to detect 2D skeletons; objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; attentionManager : to make the robot focus and follow a skeleton; cer_gaze-controller : to control the robot's gaze; motionAnalyzer --from motion-repertoire-rom12.ini : to analyze motion. This configuration file does not include the reaching exercise, which is under testing from the robot's point of view (the motion analysis is implemented); skeletonPlayer : to replay the template skeleton; skeletonScaler : to move/rescale the template skeleton; feedbackProducer : to produce a feedback depending on the exercise; actionRecognizer : to recognize the exercise being performed; feedbackSynthetizer : to generate a verbal feedback; yarpview : to visualize depth and RGB image with 2D skeletons; skeletonViewer : to visualize 3D skeletons; yarpscope : to visualize the metric on the movement in real-time; interactionManager : to supervise the interaction; ctpService (for each arm): to send commands to the robot's arms; cer_reaching-solver + cer_reaching-controller (for each arm): to let the robot performing a reaching task. When you want to stop the interaction, hit stop in yarpmanager . You can produce two final reports (in italian and english) of the performed exercises by opening a terminal and typing: assistive-rehab-generate-report.sh report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Two html files will be created in the folder where you run the script. Note Producing the report might take a while, as all the files whose names equal the name of the most recent file are processed. This allows us to create a report not only of the current session, but also of the clinical evolution of the patient.","title":"The basic app: AssistiveRehab.xml"},{"location":"main_apps/#the-basic-app-integrated-with-face-recognition-assistiverehab-facesxml","text":"The AssistiveRehab-faces.xml.template builds upon the basic app, introducing additional modules for face recognition, which are the following: R1-mk2 On R1-mk2, the face recognition pipeline is run on r1-torso2 , which has to be available. humanStructure : to restrict the area of recognition around human faces; caffeCoder : to extract a representation of the cropped image; linearClassifierModule : to classify the extracted features; faceRecognizer : to label human faces. For running the demo, click on Assistive_Rehab_with_Faces_App in yarpmanager , hit run and then connect. Note caffeCoder takes a while to load the network. Therefore, before hitting connect, wait some seconds. You can click the refresh icon and check when the caffeCoder ports become green. The demo is similar to the basic one, but now the robot interacts with people using their names! Tip If you want to train the network with your face to the face database, you can use the provided API .","title":"The basic app integrated with face recognition: AssistiveRehab-faces.xml"},{"location":"replay_an_experiment/","text":"How to save and replay an experiment \u00b6 This tutorial will show you how to save and replay an experiment. Saving an experiment \u00b6 You can save an experiment by running this application , which runs several yarpdatadumper for saving: data from the camera and yarpOpenPose : 2D skeleton data, depth image and RGB image with 2D skeletons. robot joints. Saved data can be found in the folder skeletonDumper. Note You can save the data from the virtual robot by running this application . Replaying an experiment \u00b6 After saving an experiment as previously explained, this application allows you to replay it, i.e. to visualize the saved depth, the 2D skeletons, retrieve and visualize the 3D skeletons, publish and visualize the robot skeleton. Tip The application is conceived to visualize saved data. You can run any additional module that takes as input the saved data. It runs the following modules: objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; robotSkeletonPublisher : to produce 3D robot skeleton from the saved joints; skeletonViewer : to visualize 3D skeletons; yarpview : to visualize depth and RGB image with 2D skeletons. Tip If you are only interested in the visual pipeline from the camera, you can avoid running robotSkeletonPublisher . To run the application, you need a yarpserver . Open a terminal and type: yarpserver You need a yarpdataplayer to reproduce the dataset you have previously acquired. Open a terminal and type: yarpdataplayer An interface appears, you can click on File , then Open Directory . Click on the folder where you have saved your data (the folder should contain subfolders with 2D skeleton data, depth image and RGB image with 2D skeletons) and press play. Finally open yarpmanager , click on the Assistive_Rehab_Replay_App , hit run and then connect. Note robotSkeletonPublisher automatically connects to the robot ports. If you saved the data from the virtual robot using this app , you will need to specify the parameter --robot SIM_CER_ROBOT . Tip Replaying an experiment is an important feature, with the twofold objective of: allowing the physiotherapist to compute new metrics on the replayed experiment not run online; performing code debbuging.","title":"How to replay an experiment"},{"location":"replay_an_experiment/#how-to-save-and-replay-an-experiment","text":"This tutorial will show you how to save and replay an experiment.","title":"How to save and replay an experiment"},{"location":"replay_an_experiment/#saving-an-experiment","text":"You can save an experiment by running this application , which runs several yarpdatadumper for saving: data from the camera and yarpOpenPose : 2D skeleton data, depth image and RGB image with 2D skeletons. robot joints. Saved data can be found in the folder skeletonDumper. Note You can save the data from the virtual robot by running this application .","title":"Saving an experiment"},{"location":"replay_an_experiment/#replaying-an-experiment","text":"After saving an experiment as previously explained, this application allows you to replay it, i.e. to visualize the saved depth, the 2D skeletons, retrieve and visualize the 3D skeletons, publish and visualize the robot skeleton. Tip The application is conceived to visualize saved data. You can run any additional module that takes as input the saved data. It runs the following modules: objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; robotSkeletonPublisher : to produce 3D robot skeleton from the saved joints; skeletonViewer : to visualize 3D skeletons; yarpview : to visualize depth and RGB image with 2D skeletons. Tip If you are only interested in the visual pipeline from the camera, you can avoid running robotSkeletonPublisher . To run the application, you need a yarpserver . Open a terminal and type: yarpserver You need a yarpdataplayer to reproduce the dataset you have previously acquired. Open a terminal and type: yarpdataplayer An interface appears, you can click on File , then Open Directory . Click on the folder where you have saved your data (the folder should contain subfolders with 2D skeleton data, depth image and RGB image with 2D skeletons) and press play. Finally open yarpmanager , click on the Assistive_Rehab_Replay_App , hit run and then connect. Note robotSkeletonPublisher automatically connects to the robot ports. If you saved the data from the virtual robot using this app , you will need to specify the parameter --robot SIM_CER_ROBOT . Tip Replaying an experiment is an important feature, with the twofold objective of: allowing the physiotherapist to compute new metrics on the replayed experiment not run online; performing code debbuging.","title":"Replaying an experiment"},{"location":"tug_demo/","text":"How to run the TUG demo \u00b6 This tutorial will show you how to run the TUG demo on the real robot and within the simulation environment gazebo . Running the TUG demo on the robot \u00b6 The related application can be found here . Requirements \u00b6 The following hardware is required: NVIDIA graphics card : for running yarpOpenPose ; Mystrom wifi button : for triggering the speech pipeline; Alternatively The speech pipeline can be alternatively triggered by the hand up / down option (see below ) Setup for external microphone Setting up the robot \u00b6 Turn on the robot, by switching on cpu and motors. Open yarpmanager , click on the Cluster tab on the left. Click on the play icon below the nameserver node , then select the systems in the window underneath and click on play. R1-mk2 Specifically, R1-mk2 has the following systems available: r1-base : where yarpserver runs; r1-console-linux : where the main modules and visualizers run; r1-console-cuda : connected to an external Cuda box, where yarpOpenPose runs. Here we also deployed the Virtual Machine for the speech pipeline; r1-torso1 : where the camera runs; r1-face : where the speak and the face expression modules run; Now that all systems are running, open a terminal and type: ssh -X r1-base yarprobotinterface Caution The robot is going to move around during the demo. Therefore, after running the yarprobotinterface , be sure to unplug it from the power supply. Virtual Machine for speech interaction \u00b6 For running the speechInteraction modules, you will need access to Google Cloud API , which is currently deployed on the Ubuntu 18 Virtual Machine (VM) installed on r1-console-cuda . On this system, open the VM and start Ubuntu18-VM . When the system is on, open yarpmanager and run the Google_Speech_Processing app. Hit play and then connect. Warning Be sure that the external microphone is connected to the system. Running the demo \u00b6 Now we are ready to run the demo! In yarpmanager , click on Applications and then Assistive Rehabilitation TUG App app, hit run and then connect. The demo is now running! Before starting the TUG, we need to define our world made of lines, robots and skeletons! For doing so, you can follow this tutorial . Let's run a TUG session! Want to ask questions to the robot? Remember to be equipped the WiFi button, the external microphone and the Virtual Machine running. Alternative to WiFi button The speech pipeline can be alternatively triggered by the hand up / down option: the 3D skeleton is analyzed to detect if the hand is raised up / dropped down and consequently a start / stop is sent to the speech pipeline (the hand has to stay up during the question). This can be achieved by running managerTUG with the option --detect-hand-up true . However, such approach is less robust than using the button for two reasons: the person is assumed to be always in the FOV, which cannot be guaranteed in a HRI scenario; it may lead to false detection and thus trigger the speech when not required. Open a terminal and type: Terminal 1 yarp rpc /managerTUG/cmd:rpc start The robot will move to the starting position defined by starting-pose here . When the robot has reached this point, the interaction starts! The interaction keeps going on, with the robot keeping engaging the user. You can stop it by typing in Terminal 1 : Terminal 1 stop Running the TUG demo on gazebo \u00b6 The related application can be found here . Dependencies \u00b6 After installing assistive-rehab , you will need the following dependencies: cer : for running the gaze-controller and face expressions; navigation : for controlling the robot wheels; gazebo : for running the virtual environment; Work in progress This is a fork of gazebo which contains the required changes to the Actor class. We are working to open a pull request on the upstream repository. gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ; cer-sim : which includes the model loaded by gazebo in tug-scenario.world ; speech : for running the iSpeak module; nodejs : for handling triggers from the wifi button. Requirements \u00b6 The following hardware is required: NVIDIA graphics card : for running yarpOpenPose ; Optional \u00b6 If you want to simulate the speech interaction, you will need: access to Google Cloud API : for running the speechInteraction modules; Warning You will need to have an account to access Google Cloud API. a Mystrom wifi button . How to configure the button On the robot, the WiFi button is already configured to send triggers within the robot network. To configure your own button to run within your network, follow this tutorial . Running the demo \u00b6 Now that all dependencies and requirements are met, you are ready to run the demo in gazebo ! The first step is to open a terminal and run yarpserver . Open yarpmanager , run the AssistiveRehab-TUG_SIM App and hit connect. Important If you wish to simulate the speech interaction, be sure to run node-button.js on the machine you used to configure the wifi button, as explained here . This script is in charge of getting the POST request from the button and sending a trigger to managerTUG . The TUG scenario appears within the simulation environment, including the robot, a human model standing in front of it, a chair and two ArUco lines: Info The loaded scenario is described in tug-scenario.world . Tip By default, gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, managerTUG waits for the command start to run the TUG session, which can be sent to the rpc port /managerTUG/cmd:rpc . Once this is provided, the interaction starts and the simulated user successfully completes the test, while its step length is computed in real-time. The following video shows a successful test: Tip The demo keeps going on even after the TUG is completed, with the robot engaging the human model, which executes the TUG. If you want to stop the demo, you will need to provide a stop command to the rpc port /managerTUG/cmd:rpc . Simulating the verbal interaction \u00b6 The verbal interaction between the robot and the human model is simulated through the following steps: the Mystrom wifi button is integrated within the simulated demo: when the button is pressed, the human model / the robot (depending which one is moving) stops and a question can be \"asked\"; a question is simulated through the question-simulator.sh script. The user can choose 4 keywords, specifically speed , repetition , aid , feedback , each associated to a set of questions. The script randomly selects a question associated to the chosen keyword and sends it to googleSpeechProcess which analyses the sentence; when googleSpeechProcess provides the output to managerTUG , the human model / the robot starts walking / navigating from where it stopped. The following video shows how the verbal interaction is integrated within the demo: Simulating failures \u00b6 Thrift services are provided in order to simulate the human model failing the TUG. Two possible failures are implemented: the human model does not reach the target : the robot says that the test was not passed. This can be achieved providing the command set_target to /managerTUG/cmd:rpc , as shown in the following video: Note The target is defined with respect to the gazebo world frame, thus X pointing forward (with respect to the human model), Y pointing left, Z pointing up. the human model stops : the robot encourages the user to finish the test, reminding to push the button to ask questions. If the human model does not complete the test, the test is not passed. This can be achieved providing the command pause to /tug_input_port , as shown in the following video: Tip You can also set the time during which the human model has to be paused, by specifying the seconds after pause . For example, to pause it for 2 s , you can provide pause 2.0 to tug_input_port : after the 2 s , the human model starts walking again.","title":"How to run the TUG demo"},{"location":"tug_demo/#how-to-run-the-tug-demo","text":"This tutorial will show you how to run the TUG demo on the real robot and within the simulation environment gazebo .","title":"How to run the TUG demo"},{"location":"tug_demo/#running-the-tug-demo-on-the-robot","text":"The related application can be found here .","title":"Running the TUG demo on the robot"},{"location":"tug_demo/#requirements","text":"The following hardware is required: NVIDIA graphics card : for running yarpOpenPose ; Mystrom wifi button : for triggering the speech pipeline; Alternatively The speech pipeline can be alternatively triggered by the hand up / down option (see below ) Setup for external microphone","title":"Requirements"},{"location":"tug_demo/#setting-up-the-robot","text":"Turn on the robot, by switching on cpu and motors. Open yarpmanager , click on the Cluster tab on the left. Click on the play icon below the nameserver node , then select the systems in the window underneath and click on play. R1-mk2 Specifically, R1-mk2 has the following systems available: r1-base : where yarpserver runs; r1-console-linux : where the main modules and visualizers run; r1-console-cuda : connected to an external Cuda box, where yarpOpenPose runs. Here we also deployed the Virtual Machine for the speech pipeline; r1-torso1 : where the camera runs; r1-face : where the speak and the face expression modules run; Now that all systems are running, open a terminal and type: ssh -X r1-base yarprobotinterface Caution The robot is going to move around during the demo. Therefore, after running the yarprobotinterface , be sure to unplug it from the power supply.","title":"Setting up the robot"},{"location":"tug_demo/#virtual-machine-for-speech-interaction","text":"For running the speechInteraction modules, you will need access to Google Cloud API , which is currently deployed on the Ubuntu 18 Virtual Machine (VM) installed on r1-console-cuda . On this system, open the VM and start Ubuntu18-VM . When the system is on, open yarpmanager and run the Google_Speech_Processing app. Hit play and then connect. Warning Be sure that the external microphone is connected to the system.","title":"Virtual Machine for speech interaction"},{"location":"tug_demo/#running-the-demo","text":"Now we are ready to run the demo! In yarpmanager , click on Applications and then Assistive Rehabilitation TUG App app, hit run and then connect. The demo is now running! Before starting the TUG, we need to define our world made of lines, robots and skeletons! For doing so, you can follow this tutorial . Let's run a TUG session! Want to ask questions to the robot? Remember to be equipped the WiFi button, the external microphone and the Virtual Machine running. Alternative to WiFi button The speech pipeline can be alternatively triggered by the hand up / down option: the 3D skeleton is analyzed to detect if the hand is raised up / dropped down and consequently a start / stop is sent to the speech pipeline (the hand has to stay up during the question). This can be achieved by running managerTUG with the option --detect-hand-up true . However, such approach is less robust than using the button for two reasons: the person is assumed to be always in the FOV, which cannot be guaranteed in a HRI scenario; it may lead to false detection and thus trigger the speech when not required. Open a terminal and type: Terminal 1 yarp rpc /managerTUG/cmd:rpc start The robot will move to the starting position defined by starting-pose here . When the robot has reached this point, the interaction starts! The interaction keeps going on, with the robot keeping engaging the user. You can stop it by typing in Terminal 1 : Terminal 1 stop","title":"Running the demo"},{"location":"tug_demo/#running-the-tug-demo-on-gazebo","text":"The related application can be found here .","title":"Running the TUG demo on gazebo"},{"location":"tug_demo/#dependencies","text":"After installing assistive-rehab , you will need the following dependencies: cer : for running the gaze-controller and face expressions; navigation : for controlling the robot wheels; gazebo : for running the virtual environment; Work in progress This is a fork of gazebo which contains the required changes to the Actor class. We are working to open a pull request on the upstream repository. gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ; cer-sim : which includes the model loaded by gazebo in tug-scenario.world ; speech : for running the iSpeak module; nodejs : for handling triggers from the wifi button.","title":"Dependencies"},{"location":"tug_demo/#requirements_1","text":"The following hardware is required: NVIDIA graphics card : for running yarpOpenPose ;","title":"Requirements"},{"location":"tug_demo/#optional","text":"If you want to simulate the speech interaction, you will need: access to Google Cloud API : for running the speechInteraction modules; Warning You will need to have an account to access Google Cloud API. a Mystrom wifi button . How to configure the button On the robot, the WiFi button is already configured to send triggers within the robot network. To configure your own button to run within your network, follow this tutorial .","title":"Optional"},{"location":"tug_demo/#running-the-demo_1","text":"Now that all dependencies and requirements are met, you are ready to run the demo in gazebo ! The first step is to open a terminal and run yarpserver . Open yarpmanager , run the AssistiveRehab-TUG_SIM App and hit connect. Important If you wish to simulate the speech interaction, be sure to run node-button.js on the machine you used to configure the wifi button, as explained here . This script is in charge of getting the POST request from the button and sending a trigger to managerTUG . The TUG scenario appears within the simulation environment, including the robot, a human model standing in front of it, a chair and two ArUco lines: Info The loaded scenario is described in tug-scenario.world . Tip By default, gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, managerTUG waits for the command start to run the TUG session, which can be sent to the rpc port /managerTUG/cmd:rpc . Once this is provided, the interaction starts and the simulated user successfully completes the test, while its step length is computed in real-time. The following video shows a successful test: Tip The demo keeps going on even after the TUG is completed, with the robot engaging the human model, which executes the TUG. If you want to stop the demo, you will need to provide a stop command to the rpc port /managerTUG/cmd:rpc .","title":"Running the demo"},{"location":"tug_demo/#simulating-the-verbal-interaction","text":"The verbal interaction between the robot and the human model is simulated through the following steps: the Mystrom wifi button is integrated within the simulated demo: when the button is pressed, the human model / the robot (depending which one is moving) stops and a question can be \"asked\"; a question is simulated through the question-simulator.sh script. The user can choose 4 keywords, specifically speed , repetition , aid , feedback , each associated to a set of questions. The script randomly selects a question associated to the chosen keyword and sends it to googleSpeechProcess which analyses the sentence; when googleSpeechProcess provides the output to managerTUG , the human model / the robot starts walking / navigating from where it stopped. The following video shows how the verbal interaction is integrated within the demo:","title":"Simulating the verbal interaction"},{"location":"tug_demo/#simulating-failures","text":"Thrift services are provided in order to simulate the human model failing the TUG. Two possible failures are implemented: the human model does not reach the target : the robot says that the test was not passed. This can be achieved providing the command set_target to /managerTUG/cmd:rpc , as shown in the following video: Note The target is defined with respect to the gazebo world frame, thus X pointing forward (with respect to the human model), Y pointing left, Z pointing up. the human model stops : the robot encourages the user to finish the test, reminding to push the button to ask questions. If the human model does not complete the test, the test is not passed. This can be achieved providing the command pause to /tug_input_port , as shown in the following video: Tip You can also set the time during which the human model has to be paused, by specifying the seconds after pause . For example, to pause it for 2 s , you can provide pause 2.0 to tug_input_port : after the 2 s , the human model starts walking again.","title":"Simulating failures"},{"location":"tug_lines/","text":"How to detect TUG start and finish lines \u00b6 This tutorial will show you how to detect start and finish lines defined for the TUG on the robot and in gazebo . Using ArUco boards \u00b6 In order to identify a line, we rely on ArUco boards provided by OpenCV , which are sets of markers whose relative position is known a-priori. OpenCV provides the API to easily create and detect such boards. Tip A board compared to a set of independent markers has the advantage of providing an estimated pose usually more accurate, which can be computed even in presence of occlusions or partial views (not all markers are required to perform pose estimation). For the TUG, we define two lines, which are ArUco boards with a single line of 6 markers, belonging to two different marker dictionaries and thus uniquely identified. Specifically: the start-line : where the user's chair is placed; the finish-line : which the user has to cross; Info Check out more on markers and dictionaries here . The lines we use can be found here . Each line is composed of two images, ending with _0 and _1 . You will need to print them separately and then stick them together as following: start-line finish-line origin in the bottom left corner x (red) pointing along the line's length y (green) pointing along the width z (blue) pointing out of the line Hint This is required to have a board big enough to be robustly detected in the image, given we use a camera resolution of 320x240 px . Rationale: a world made of lines, robots and skeletons \u00b6 Why do we need such lines? The TUG itself requires a finish line indicating the end of the path to cross before going back to the chair. We decided to rely on robot's perception and visually identify such line only once before starting the exercise. Such information feeds our database and thus the information is kept even if / when the line is not visible. When the odometry starts, the robot is assumed to be in the origin, which is however a random point in the world. In order to have a starting reference point for the odometry, we further introduced an additional line, the start-line : when such line is detected, the line frame becomes the world, with x pointing along the line length, y along its width and z out of the line, and robot and skeletons are referred to it. The following shows an example of our world: Detecting lines on the robot \u00b6 Assumptions We assume the robot is running, with the motors on. On the robot assistive-rehab and its dependencies are already set, with the required applications in the yarpmanager . However, you can check requirements and installation instructions here . We will need to move the robot around and for doing so we can use the joystick to control it. Turn on the joystick, open yarpmanager and click R1_joystick_mobile_base_control . Hit run and connect. Info The required app can be found here . Alternatively Open yarpmotorgui --robot cer --parts \"(mobile_base)\" and click on the yellow pause icon to idle the wheels. Now you will be able to manually move the robot around. In the yarpmanager , click on Assistive Rehabilitation lineDetector App , hit run and connect. Note If you are already running the demo, you can skip this last step: all the required modules are running already. Info navController and cer_gaze_controller are required from lineDetector to estimate the robot position with respect to the start line. Next step is to adjust the robot's head to have the lines in the field of view. Therefore, open a terminal and type: yarpmotorgui --robot cer --parts \"(head)\" This will open the GUI where you will be able to directly control the head of the robot. Move the first joint (pitch) until the line is visible in yarpview (up to ~ 20 degree ). Now use the joystick to move the robot in front of the start-line . Mind the order The order of detection is important: since the start-line is used as reference frame, this has to be detected first. Open a terminal and type: Terminal 1 yarp rpc /lineDetector/cmd:rpc detect start-line Before starting the detection, the robot is in a random position: when the detection starts, the estimated pose appears on the yarp viewer; when the ack is provided, the detection is complete: the pose is estimated and the line appears on the skeletonViewer , with the robot position recalculated with respect to the line. From now on, the start line is the origin of the world and robot and skeletons positions are referred to it. Note The estimated pose is filtered by means of a median filter, thus the detection takes few seconds. We need now to detect the finish-line . As before, move the robot in front of the line and in Terminal 1 , type: Terminal 1 detect finish-line Success When the command provides an ack , the detection is complete and the finish line appears on skeletonViewer . Important The robot navigates the environment, reaching the desired targets with errors accepted within some tolerances (defined in navController ). Such errors accumulate over time and after a while it might be necessary to update the robot position (the real robot position drifts from what you see on skeletonViewer ). For doing so, you can physically move the robot to one of the lines and run the command update_odometry , specifying the line's tag and the robot orientation with respect to the start-line : skeletonViewer will show the updated position. Detecting lines in gazebo \u00b6 Now we are going to see how to detect these lines in gazebo . Dependencies \u00b6 After installing assistive-rehab and its requirements, you will need the following dependencies: gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ; cer-sim : which includes the robot model loaded by gazebo in AssistiveRehab-TUG_SIM.xml.template ; cer : for running the cer_gaze-controller ; navigation : for running baseControl . Preparing your environment \u00b6 The first step you need to take is to prepare your environment. The folder assistive-rehab/app/gazebo/tug includes the sdf models of the Aruco boards (namely aruco_board_start and aruco_board_finish ) and the world including the robot and the lines (namely tug-scenario.world ). Following this gazebo tutorial, you need to let gazebo know where models and worlds are located. For doing this, you will need to set the following environment variables: GAZEBO_MODEL_PATH : has to point to the folder including the models ( assistive-rehab/app/gazebo/tug ); GAZEBO_RESOURCE_PATH : has to point to the folder including the world application ( assistive-rehab/app/gazebo/tug ); Running the application \u00b6 In assistive-rehab/app/scripts you will find the application AssistiveRehab-TUG_SIM.xml.template to run the TUG demo in gazebo . First, you need to run yarpserver , opening a terminal and typing: yarpserver Open yarpmanager , run the AssistiveRehab-TUG_SIM.xml and hit connect. Warning The xml is conceived for the complete TUG demo, therefore not all modules are required for this specific application. In the complete application, lineDetector takes the RGB input propagated from yarpOpenPose . For this specific application, you don't need to run yarpOpenPose and can simply connect /SIM_CER_ROBOT/depthCamera/rgbImage:o to /lineDetector/img:i . You will need to run the following modules: gazebo : to run gazebo and load the world; objectsPropertiesCollector : to store the robot skeleton and the lines within a yarp-oriented database; robotSkeletonPublisher : to publish the skeleton representing the robot; baseControl : to control the robot's base; navController : to send commands to baseControl ; cer_gaze-controller : to control the robot's gaze; lineDetector : to detect the lines; yarpview --name /viewer/line : to visualize the detected lines in the RGB image; skeletonViewer : to visualize the detected lines and the robot skeleton in the world. Tip You can customize the app by removing unnecessary modules. You can save the app in your folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , your customized app will be automatically loaded. You should now see the gazebo GUI with the robot and the two Aruco boards loaded, as following: You can see there are two Aruco boards: we identify the closer to the robot as finish-line and the further as start-line . Let's detect these lines! You will need to move the robot's head to have the lines in the field of view. Therefore, open a terminal and type: yarpmotorgui --robot SIM_CER_ROBOT --parts \"(head)\" As with the real robot, move the first joint (pitch) until the line is visible in yarpview (up to ~ 20 degree ). Since we don't have a joystick to control the robot in gazebo , we can rely on navController for moving it. You will then need to send commands to navController ( Terminal 1 ) for moving the robot and lineDetector ( Terminal 2 ) for detecting the lines. Therefore, open two terminals and type: Terminal 1 yarp rpc /navController/rpc Terminal 2 yarp rpc /lineDetector/cmd:rpc Note From now on, I will refer to Terminal 1 for commands provided to navController and Terminal 2 for commands to lineDetector . Let's start to detect the start-line . When the application starts, the start line is out of the robot's field of view. Therefore you have to move the robot to a point of the world where the line is visible. For doing so, you can use the go_to service provided by navController , typing the command shown in Terminal 1 . This will move the robot 2.5 meters forward keeping its current orientation and you will see the robot approaching the line. When the robot stops, the line will be in the robot's field of view and you can detect it through the detect service provided by lineDetector , typing the command shown in Terminal 2 : Note When the application starts, the robot reference frame follows the following convention: X pointing forward, Y pointing left, Z pointing up. Terminal 1 (navController) go_to 2.5 0.0 0.0 Terminal 2 (lineDetector) detect start-line Tip navController and lineDetector provide services through thrift . Therefore, you can check what's required by the specific service typing help followed by the name of the service (for example help go_to ). This is what you should see: As with the real robot, when the start line is estimated it appears in the skeletonViewer and becomes the origin of the world: the robot position in the skeletonViewer is recalculated with respect to the line. Let's now detect the finish-line . The procedure is the same as before, therefore you will have to move to robot to a point of the world where the finish line is visible. Warning Keep in mind that the robot position is now referred to the start line. Therefore the robot reference frame is: X pointing right, Y pointing forward, Z pointing up. Again, use the go_to service provided by navController , typing the command shown in Terminal 1 . The robot will start approaching the line. When the robot stops, you can detect the finish line through the detect service provided by lineDetector , typing the command shown in Terminal 2 : Terminal 1 (navController) go_to 0.0 -5.5 90.0 true Terminal 2 (lineDetector) detect finish-line Tip The true flag in Terminal 1 allows the robot to move backward. This is what you should see: Finally, to check that the line is estimated correctly, you can use the go_to_line service provided by lineDetector , as shown in Terminal 2 : Terminal 2 (lineDetector) go_to_line finish-line You will see the robot going to the finish line origin:","title":"How to detect the TUG lines"},{"location":"tug_lines/#how-to-detect-tug-start-and-finish-lines","text":"This tutorial will show you how to detect start and finish lines defined for the TUG on the robot and in gazebo .","title":"How to detect TUG start and finish lines"},{"location":"tug_lines/#using-aruco-boards","text":"In order to identify a line, we rely on ArUco boards provided by OpenCV , which are sets of markers whose relative position is known a-priori. OpenCV provides the API to easily create and detect such boards. Tip A board compared to a set of independent markers has the advantage of providing an estimated pose usually more accurate, which can be computed even in presence of occlusions or partial views (not all markers are required to perform pose estimation). For the TUG, we define two lines, which are ArUco boards with a single line of 6 markers, belonging to two different marker dictionaries and thus uniquely identified. Specifically: the start-line : where the user's chair is placed; the finish-line : which the user has to cross; Info Check out more on markers and dictionaries here . The lines we use can be found here . Each line is composed of two images, ending with _0 and _1 . You will need to print them separately and then stick them together as following: start-line finish-line origin in the bottom left corner x (red) pointing along the line's length y (green) pointing along the width z (blue) pointing out of the line Hint This is required to have a board big enough to be robustly detected in the image, given we use a camera resolution of 320x240 px .","title":"Using ArUco boards"},{"location":"tug_lines/#rationale-a-world-made-of-lines-robots-and-skeletons","text":"Why do we need such lines? The TUG itself requires a finish line indicating the end of the path to cross before going back to the chair. We decided to rely on robot's perception and visually identify such line only once before starting the exercise. Such information feeds our database and thus the information is kept even if / when the line is not visible. When the odometry starts, the robot is assumed to be in the origin, which is however a random point in the world. In order to have a starting reference point for the odometry, we further introduced an additional line, the start-line : when such line is detected, the line frame becomes the world, with x pointing along the line length, y along its width and z out of the line, and robot and skeletons are referred to it. The following shows an example of our world:","title":"Rationale: a world made of lines, robots and skeletons"},{"location":"tug_lines/#detecting-lines-on-the-robot","text":"Assumptions We assume the robot is running, with the motors on. On the robot assistive-rehab and its dependencies are already set, with the required applications in the yarpmanager . However, you can check requirements and installation instructions here . We will need to move the robot around and for doing so we can use the joystick to control it. Turn on the joystick, open yarpmanager and click R1_joystick_mobile_base_control . Hit run and connect. Info The required app can be found here . Alternatively Open yarpmotorgui --robot cer --parts \"(mobile_base)\" and click on the yellow pause icon to idle the wheels. Now you will be able to manually move the robot around. In the yarpmanager , click on Assistive Rehabilitation lineDetector App , hit run and connect. Note If you are already running the demo, you can skip this last step: all the required modules are running already. Info navController and cer_gaze_controller are required from lineDetector to estimate the robot position with respect to the start line. Next step is to adjust the robot's head to have the lines in the field of view. Therefore, open a terminal and type: yarpmotorgui --robot cer --parts \"(head)\" This will open the GUI where you will be able to directly control the head of the robot. Move the first joint (pitch) until the line is visible in yarpview (up to ~ 20 degree ). Now use the joystick to move the robot in front of the start-line . Mind the order The order of detection is important: since the start-line is used as reference frame, this has to be detected first. Open a terminal and type: Terminal 1 yarp rpc /lineDetector/cmd:rpc detect start-line Before starting the detection, the robot is in a random position: when the detection starts, the estimated pose appears on the yarp viewer; when the ack is provided, the detection is complete: the pose is estimated and the line appears on the skeletonViewer , with the robot position recalculated with respect to the line. From now on, the start line is the origin of the world and robot and skeletons positions are referred to it. Note The estimated pose is filtered by means of a median filter, thus the detection takes few seconds. We need now to detect the finish-line . As before, move the robot in front of the line and in Terminal 1 , type: Terminal 1 detect finish-line Success When the command provides an ack , the detection is complete and the finish line appears on skeletonViewer . Important The robot navigates the environment, reaching the desired targets with errors accepted within some tolerances (defined in navController ). Such errors accumulate over time and after a while it might be necessary to update the robot position (the real robot position drifts from what you see on skeletonViewer ). For doing so, you can physically move the robot to one of the lines and run the command update_odometry , specifying the line's tag and the robot orientation with respect to the start-line : skeletonViewer will show the updated position.","title":"Detecting lines on the robot"},{"location":"tug_lines/#detecting-lines-in-gazebo","text":"Now we are going to see how to detect these lines in gazebo .","title":"Detecting lines in gazebo"},{"location":"tug_lines/#dependencies","text":"After installing assistive-rehab and its requirements, you will need the following dependencies: gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in gazebo ; cer-sim : which includes the robot model loaded by gazebo in AssistiveRehab-TUG_SIM.xml.template ; cer : for running the cer_gaze-controller ; navigation : for running baseControl .","title":"Dependencies"},{"location":"tug_lines/#preparing-your-environment","text":"The first step you need to take is to prepare your environment. The folder assistive-rehab/app/gazebo/tug includes the sdf models of the Aruco boards (namely aruco_board_start and aruco_board_finish ) and the world including the robot and the lines (namely tug-scenario.world ). Following this gazebo tutorial, you need to let gazebo know where models and worlds are located. For doing this, you will need to set the following environment variables: GAZEBO_MODEL_PATH : has to point to the folder including the models ( assistive-rehab/app/gazebo/tug ); GAZEBO_RESOURCE_PATH : has to point to the folder including the world application ( assistive-rehab/app/gazebo/tug );","title":"Preparing your environment"},{"location":"tug_lines/#running-the-application","text":"In assistive-rehab/app/scripts you will find the application AssistiveRehab-TUG_SIM.xml.template to run the TUG demo in gazebo . First, you need to run yarpserver , opening a terminal and typing: yarpserver Open yarpmanager , run the AssistiveRehab-TUG_SIM.xml and hit connect. Warning The xml is conceived for the complete TUG demo, therefore not all modules are required for this specific application. In the complete application, lineDetector takes the RGB input propagated from yarpOpenPose . For this specific application, you don't need to run yarpOpenPose and can simply connect /SIM_CER_ROBOT/depthCamera/rgbImage:o to /lineDetector/img:i . You will need to run the following modules: gazebo : to run gazebo and load the world; objectsPropertiesCollector : to store the robot skeleton and the lines within a yarp-oriented database; robotSkeletonPublisher : to publish the skeleton representing the robot; baseControl : to control the robot's base; navController : to send commands to baseControl ; cer_gaze-controller : to control the robot's gaze; lineDetector : to detect the lines; yarpview --name /viewer/line : to visualize the detected lines in the RGB image; skeletonViewer : to visualize the detected lines and the robot skeleton in the world. Tip You can customize the app by removing unnecessary modules. You can save the app in your folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , your customized app will be automatically loaded. You should now see the gazebo GUI with the robot and the two Aruco boards loaded, as following: You can see there are two Aruco boards: we identify the closer to the robot as finish-line and the further as start-line . Let's detect these lines! You will need to move the robot's head to have the lines in the field of view. Therefore, open a terminal and type: yarpmotorgui --robot SIM_CER_ROBOT --parts \"(head)\" As with the real robot, move the first joint (pitch) until the line is visible in yarpview (up to ~ 20 degree ). Since we don't have a joystick to control the robot in gazebo , we can rely on navController for moving it. You will then need to send commands to navController ( Terminal 1 ) for moving the robot and lineDetector ( Terminal 2 ) for detecting the lines. Therefore, open two terminals and type: Terminal 1 yarp rpc /navController/rpc Terminal 2 yarp rpc /lineDetector/cmd:rpc Note From now on, I will refer to Terminal 1 for commands provided to navController and Terminal 2 for commands to lineDetector . Let's start to detect the start-line . When the application starts, the start line is out of the robot's field of view. Therefore you have to move the robot to a point of the world where the line is visible. For doing so, you can use the go_to service provided by navController , typing the command shown in Terminal 1 . This will move the robot 2.5 meters forward keeping its current orientation and you will see the robot approaching the line. When the robot stops, the line will be in the robot's field of view and you can detect it through the detect service provided by lineDetector , typing the command shown in Terminal 2 : Note When the application starts, the robot reference frame follows the following convention: X pointing forward, Y pointing left, Z pointing up. Terminal 1 (navController) go_to 2.5 0.0 0.0 Terminal 2 (lineDetector) detect start-line Tip navController and lineDetector provide services through thrift . Therefore, you can check what's required by the specific service typing help followed by the name of the service (for example help go_to ). This is what you should see: As with the real robot, when the start line is estimated it appears in the skeletonViewer and becomes the origin of the world: the robot position in the skeletonViewer is recalculated with respect to the line. Let's now detect the finish-line . The procedure is the same as before, therefore you will have to move to robot to a point of the world where the finish line is visible. Warning Keep in mind that the robot position is now referred to the start line. Therefore the robot reference frame is: X pointing right, Y pointing forward, Z pointing up. Again, use the go_to service provided by navController , typing the command shown in Terminal 1 . The robot will start approaching the line. When the robot stops, you can detect the finish line through the detect service provided by lineDetector , typing the command shown in Terminal 2 : Terminal 1 (navController) go_to 0.0 -5.5 90.0 true Terminal 2 (lineDetector) detect finish-line Tip The true flag in Terminal 1 allows the robot to move backward. This is what you should see: Finally, to check that the line is estimated correctly, you can use the go_to_line service provided by lineDetector , as shown in Terminal 2 : Terminal 2 (lineDetector) go_to_line finish-line You will see the robot going to the finish line origin:","title":"Running the application"},{"location":"tutorial_intro/","text":"Getting started \u00b6 If you want to learn how to exploit our framework to create something similar for your own applications, you can start from here: How to manage a skeleton object How to replay an experiment How to run the visual pipeline in a disembodied manner How to temporally align two signals How to run the rehabilitation demos How to run the virtual demo How to detect TUG start and finish lines How to run the TUG demo How to use the gazebo plugin How to configure the wifi button","title":"Getting started"},{"location":"tutorial_intro/#getting-started","text":"If you want to learn how to exploit our framework to create something similar for your own applications, you can start from here: How to manage a skeleton object How to replay an experiment How to run the visual pipeline in a disembodied manner How to temporally align two signals How to run the rehabilitation demos How to run the virtual demo How to detect TUG start and finish lines How to run the TUG demo How to use the gazebo plugin How to configure the wifi button","title":"Getting started"},{"location":"virtual_demo/","text":"How to run the virtual demo \u00b6 This tutorial will show you how to run the virtual demo. The virtual demo replicates the demo with the real R1 in a virtual environment ( Gazebo ), with the virtual version of the robot: The virtual R1 is shown on a screen with a RealSense on the top (indicated by the red arrow in the picture). In this demo, the user in the field of view is automatically engaged and the interaction includes three phases: observation phase : the virtual robot welcomes the user and shows the exercise to perform; direct imitation phase : the virtual robot performs the exercise together with the user, while providing a verbal feedback on how the exercise is being performed; occluded imitation phase : the virtual robot keeps performing the exercise behind a panel and stops providing the verbal feedback . Features like facial expressions, gazing the user and a verbal feedback are also included, such that the interaction is as close as possible to the real one. The related application can be found here , named AssistiveRehab-TWM-virtual.xml.template . The Train with Me study This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper . Dependencies \u00b6 After installing assistive-rehab , you will need the following dependencies: RealSense : for running the RealSense; cer : for running the gaze-controller and face expressions; gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in Gazebo ; cer-sim : which includes the model loaded by Gazebo in AssistiveRehab-TWM-virtual.xml.template ; speech : for running the iSpeak module. Requirements \u00b6 The following hardware is required: RealSense camera ; NVIDIA graphics card : for running yarpOpenPose and actionRecognizer . Run the virtual demo \u00b6 To run the demo, first run yarpserver . Connect the RealSense to your laptop. Open yarpmanager , run the AssistiveRehab-TWM-virtual App and connect. A virtual R1 appears within the simulation environment. Note By default, Gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, the interactionManager waits for the command start_observation to start the exercise session. Specifically, the following commands should be sent in this order to the rpc port /interactionManager/cmd:rpc : start_observation : to start the observation phase, where the robot looks for a user and, when it founds her/him, starts showing the exercise to be performed; start_imitation : to start the direct imitation phase, where the robot performs the exercise together with the user, while providing her/him with verbal feedback; start_occlusion : to start the occluded imitation phase, where the robot keeps performing the exercise behind a panel and stops providing the verbal feedback; stop : to stop the interaction. Tip A different kind of interaction is also allowed, requiring the user to raise her/his hand to start the interaction. Such interaction includes the observation and the direct imitation phases, with the virtual robot showing the exercise to perform and, right after, performing the exercise while providing a verbal feedback (this is the virtual version of the demo Y1M5 ). In this configuration, the command start_with_hand starts the interaction and, when the user raises her/his hand, the two phases are run in a row, one after the other, without waiting any additional command. The following picture shows an example of interaction during the direct and occluded imitation phase: During the direct imitation phase, the robot moves while providing verbal feedback (the mouth appears on the robot's screen). During the occluded imitation phase, a panel appears occluding the moving arm and the robot stops providing verbal feedback (the mouth only appears at the end of the exercise, when the robot warns the user that the exercise is over). Note The shown interaction is just an example and the number of repetitions of the exercise is higher by default. The parameters used for this application can be found in the context train-with-me . Specifically, parameters that control the repetitions of the arm movements are defined in the interactionManager.ini as nrep-show and nrep-perform , respectively for the observation phase (set to 8 by default) and the imitation phase, both direct and occluded (set to 16 by default). The command start_occlusion can be sent after the eighth repetition of the movement within the direct phase, to have 8 repetitions for each phase.","title":"How to run the virtual demo"},{"location":"virtual_demo/#how-to-run-the-virtual-demo","text":"This tutorial will show you how to run the virtual demo. The virtual demo replicates the demo with the real R1 in a virtual environment ( Gazebo ), with the virtual version of the robot: The virtual R1 is shown on a screen with a RealSense on the top (indicated by the red arrow in the picture). In this demo, the user in the field of view is automatically engaged and the interaction includes three phases: observation phase : the virtual robot welcomes the user and shows the exercise to perform; direct imitation phase : the virtual robot performs the exercise together with the user, while providing a verbal feedback on how the exercise is being performed; occluded imitation phase : the virtual robot keeps performing the exercise behind a panel and stops providing the verbal feedback . Features like facial expressions, gazing the user and a verbal feedback are also included, such that the interaction is as close as possible to the real one. The related application can be found here , named AssistiveRehab-TWM-virtual.xml.template . The Train with Me study This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper .","title":"How to run the virtual demo"},{"location":"virtual_demo/#dependencies","text":"After installing assistive-rehab , you will need the following dependencies: RealSense : for running the RealSense; cer : for running the gaze-controller and face expressions; gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in Gazebo ; cer-sim : which includes the model loaded by Gazebo in AssistiveRehab-TWM-virtual.xml.template ; speech : for running the iSpeak module.","title":"Dependencies"},{"location":"virtual_demo/#requirements","text":"The following hardware is required: RealSense camera ; NVIDIA graphics card : for running yarpOpenPose and actionRecognizer .","title":"Requirements"},{"location":"virtual_demo/#run-the-virtual-demo","text":"To run the demo, first run yarpserver . Connect the RealSense to your laptop. Open yarpmanager , run the AssistiveRehab-TWM-virtual App and connect. A virtual R1 appears within the simulation environment. Note By default, Gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, the interactionManager waits for the command start_observation to start the exercise session. Specifically, the following commands should be sent in this order to the rpc port /interactionManager/cmd:rpc : start_observation : to start the observation phase, where the robot looks for a user and, when it founds her/him, starts showing the exercise to be performed; start_imitation : to start the direct imitation phase, where the robot performs the exercise together with the user, while providing her/him with verbal feedback; start_occlusion : to start the occluded imitation phase, where the robot keeps performing the exercise behind a panel and stops providing the verbal feedback; stop : to stop the interaction. Tip A different kind of interaction is also allowed, requiring the user to raise her/his hand to start the interaction. Such interaction includes the observation and the direct imitation phases, with the virtual robot showing the exercise to perform and, right after, performing the exercise while providing a verbal feedback (this is the virtual version of the demo Y1M5 ). In this configuration, the command start_with_hand starts the interaction and, when the user raises her/his hand, the two phases are run in a row, one after the other, without waiting any additional command. The following picture shows an example of interaction during the direct and occluded imitation phase: During the direct imitation phase, the robot moves while providing verbal feedback (the mouth appears on the robot's screen). During the occluded imitation phase, a panel appears occluding the moving arm and the robot stops providing verbal feedback (the mouth only appears at the end of the exercise, when the robot warns the user that the exercise is over). Note The shown interaction is just an example and the number of repetitions of the exercise is higher by default. The parameters used for this application can be found in the context train-with-me . Specifically, parameters that control the repetitions of the arm movements are defined in the interactionManager.ini as nrep-show and nrep-perform , respectively for the observation phase (set to 8 by default) and the imitation phase, both direct and occluded (set to 16 by default). The command start_occlusion can be sent after the eighth repetition of the movement within the direct phase, to have 8 repetitions for each phase.","title":"Run the virtual demo"},{"location":"wifi_button/","text":"How to configure the wifi button \u00b6 This tutorial will show you how to configure the Mystrom wifi button . Important This is required if you want to run the TUG demo with the speech interaction. Using the app \u00b6 First, you will need to download the app and create an account. Open the app, click on the symbol in the right corner and click on Add device / WiFi Button . Now we will connect the button to your wifi network. Click on Not connected and select Light . Choose the manual configuration. Go on until this screen appers: Press the button for 1 second and the button will start blinking white. Go on, this screen will appear: Keep the button pressed for 2 seconds, the LED will start blinking red and white: now the button will be in Access Point for 5 minutes. Be patient This might take few seconds. Go on and select and connect to my-button-XXXXXX network, where XXXXXX will change depending on the button. Go back to the app and wait until a list of wifi networks appears. Choose yours and click on Use fixed IP . Now choose an IP address for your button and insert your subnet mask, gateway and DNS. If required insert the password of your network. How to set the IP address The IP address you choose must be in the same subnet of the machine you will interface to. Thus the first three numbers of the IP address you choose should match those of your machine. Follow this guide to know the IP address of your machine. After few seconds the wifi button is found and connected to your network! Go on and choose a name for it and an icon. Done? Congratulations, now your button configured!! You can skip the action setting as I will show you how so set actions in next section. Setting the actions \u00b6 This is a guide that shows you how to communicate with your device once it has been configured! To get the device specific information, you can do: curl --location --request GET '[Button IP]/api/v1/device' Tip Replace [Button IP] with the one you chose when configuring the button. The output will be a json object with the mac address (without delimiters) and its field, as shown here . Now we want to set the button such that it sends a POST request to our IP if pressed once, which corresponds to the single action. Additional actions For the TUG demo , we only use the single action. As explained here , this button also allows you to set double (http request executed when pressing the button twice) and long (http request executed when pressing the button long). Not familiar with http requests? Check this out! For doing so, you can do: curl -v -d \"single=post://[Your IP]/api/mystrom?key%3Dspeech\" http://[Button IP]/api/v1/device/[Button MAC] where: you need to replace [Your IP] with the IP address of the machine where you want to receive the POST request; you need to replace [Button IP] with the IP address of your button; you need to replace [Button MAC] with the MAC address of your button; the string speech after key%3D is the value of the keyword sent with the POST request. Button MAC You can find the MAC address of your button in the app: click on the button, go on Settings / Technical specifications . Note that in the curl command, you need to insert the MAC address with no semicolons. Connection refused If you get the following error: Failed to connect to [Button IP] port 80: No route to host , it means that the device is into sleep mode. This is done in order to preserve battery life (the device is visible in the network only after adding to wifi). Luckily, you can enter into maintenance mode, by keeping the button pressed for 5 seconds (it starts blinking green). Now you should be able to communicate again with the device! In this way, we set the single action, which corresponds to the http request executed when pressing the button once. Important If you want to use the wifi button to run the TUG demo , configure the single action to send a POST request to the machine where you run node-button.js . Tip To set double or long action, you can follow the same procedure and replace in the curl command single with the action you want to configure. Now if you get the device specific information, using the command: curl --location --request GET '[Button IP]/api/v1/device' you will see a json output: { \"[Button MAC]\" : { \"type\" : \"button\" , \"battery\" : true , \"reachable\" : true , \"meshroot\" : false , \"charge\" : false , \"voltage\" : 4.179 , \"fw_version\" : \"2.74.31\" , \"single\" : \"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\" , \"double\" : \"\" , \"long\" : \"\" , \"touch\" : \"\" , \"generic\" : \"\" , \"connectionStatus\" : { \"ntp\" : true , \"dns\" : true , \"connection\" : true , \"handshake\" : true , \"login\" : true }, \"name\" : \"\" } } where the single action is configured as \"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\" .","title":"How to configure the wifi button"},{"location":"wifi_button/#how-to-configure-the-wifi-button","text":"This tutorial will show you how to configure the Mystrom wifi button . Important This is required if you want to run the TUG demo with the speech interaction.","title":"How to configure the wifi button"},{"location":"wifi_button/#using-the-app","text":"First, you will need to download the app and create an account. Open the app, click on the symbol in the right corner and click on Add device / WiFi Button . Now we will connect the button to your wifi network. Click on Not connected and select Light . Choose the manual configuration. Go on until this screen appers: Press the button for 1 second and the button will start blinking white. Go on, this screen will appear: Keep the button pressed for 2 seconds, the LED will start blinking red and white: now the button will be in Access Point for 5 minutes. Be patient This might take few seconds. Go on and select and connect to my-button-XXXXXX network, where XXXXXX will change depending on the button. Go back to the app and wait until a list of wifi networks appears. Choose yours and click on Use fixed IP . Now choose an IP address for your button and insert your subnet mask, gateway and DNS. If required insert the password of your network. How to set the IP address The IP address you choose must be in the same subnet of the machine you will interface to. Thus the first three numbers of the IP address you choose should match those of your machine. Follow this guide to know the IP address of your machine. After few seconds the wifi button is found and connected to your network! Go on and choose a name for it and an icon. Done? Congratulations, now your button configured!! You can skip the action setting as I will show you how so set actions in next section.","title":"Using the app"},{"location":"wifi_button/#setting-the-actions","text":"This is a guide that shows you how to communicate with your device once it has been configured! To get the device specific information, you can do: curl --location --request GET '[Button IP]/api/v1/device' Tip Replace [Button IP] with the one you chose when configuring the button. The output will be a json object with the mac address (without delimiters) and its field, as shown here . Now we want to set the button such that it sends a POST request to our IP if pressed once, which corresponds to the single action. Additional actions For the TUG demo , we only use the single action. As explained here , this button also allows you to set double (http request executed when pressing the button twice) and long (http request executed when pressing the button long). Not familiar with http requests? Check this out! For doing so, you can do: curl -v -d \"single=post://[Your IP]/api/mystrom?key%3Dspeech\" http://[Button IP]/api/v1/device/[Button MAC] where: you need to replace [Your IP] with the IP address of the machine where you want to receive the POST request; you need to replace [Button IP] with the IP address of your button; you need to replace [Button MAC] with the MAC address of your button; the string speech after key%3D is the value of the keyword sent with the POST request. Button MAC You can find the MAC address of your button in the app: click on the button, go on Settings / Technical specifications . Note that in the curl command, you need to insert the MAC address with no semicolons. Connection refused If you get the following error: Failed to connect to [Button IP] port 80: No route to host , it means that the device is into sleep mode. This is done in order to preserve battery life (the device is visible in the network only after adding to wifi). Luckily, you can enter into maintenance mode, by keeping the button pressed for 5 seconds (it starts blinking green). Now you should be able to communicate again with the device! In this way, we set the single action, which corresponds to the http request executed when pressing the button once. Important If you want to use the wifi button to run the TUG demo , configure the single action to send a POST request to the machine where you run node-button.js . Tip To set double or long action, you can follow the same procedure and replace in the curl command single with the action you want to configure. Now if you get the device specific information, using the command: curl --location --request GET '[Button IP]/api/v1/device' you will see a json output: { \"[Button MAC]\" : { \"type\" : \"button\" , \"battery\" : true , \"reachable\" : true , \"meshroot\" : false , \"charge\" : false , \"voltage\" : 4.179 , \"fw_version\" : \"2.74.31\" , \"single\" : \"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\" , \"double\" : \"\" , \"long\" : \"\" , \"touch\" : \"\" , \"generic\" : \"\" , \"connectionStatus\" : { \"ntp\" : true , \"dns\" : true , \"connection\" : true , \"handshake\" : true , \"login\" : true }, \"name\" : \"\" } } where the single action is configured as \"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\" .","title":"Setting the actions"}]}